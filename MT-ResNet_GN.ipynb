{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet201\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Add, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Layer, MaxPool2D, ReLU, Resizing\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_dir = 'C:\\\\Users\\\\Thomas Wang\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\SCUT-FBP5500_v2\\\\'\n",
    "scut_dir = 'C:\\\\Users\\\\Thomas Wang\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\SCUT-FBP\\\\'\n",
    "mebeauty_dir = 'C:\\\\Users\\\\Thomas Wang\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\MEBeauty\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Background from Images\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "file_list = glob.glob(dataset_dir + 'Images\\\\*')\n",
    "\n",
    "BG_COLOR = (255, 255, 255)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        file_name = file_list[i].split('\\\\')[-1]\n",
    "        image = cv2.imread(file_list[i])\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "        \n",
    "        #fg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        #fg_image[:] = MASK_COLOR\n",
    "        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        bg_image[:] = BG_COLOR\n",
    "\n",
    "        output_image = np.where(condition, image, bg_image)\n",
    "        cv2.imwrite(dataset_dir + 'Cleaned_Images\\\\' + file_name, output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation - MT-ResNet Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv(dataset_dir + 'train_test_files\\\\All_labels.txt', sep = ' ', header = None)\n",
    "all_ratings.columns = ['img_path', 'rating']\n",
    "\n",
    "img_arr = np.zeros([len(all_ratings), 224, 224, 3])\n",
    "y_arr = np.zeros([len(all_ratings), 2])\n",
    "\n",
    "for i in tqdm(range(len(all_ratings))):\n",
    "    file_name = all_ratings.iloc[i, 0]\n",
    "    \n",
    "    race = file_name[0]\n",
    "    gender = file_name[1]\n",
    "    \n",
    "    if gender == 'M':\n",
    "        y_arr[i, 1] = 0\n",
    "    elif gender == 'F':\n",
    "        y_arr[i, 1] = 1\n",
    "        \n",
    "    y_arr[i, 0] = all_ratings.iloc[i, 1]\n",
    "    \n",
    "    img = tf.io.read_file(dataset_dir + 'Cleaned_Images\\\\' + file_name)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.keras.layers.Resizing(224, 224)(img)\n",
    "    img /= 255\n",
    "    img_arr[i] = img\n",
    "    \n",
    "np.save(dataset_dir + 'train_img_arr.npy', img_arr)\n",
    "np.save(dataset_dir + 'train_y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_arr = np.load(dataset_dir + 'train_img_arr.npy')\n",
    "train_y_arr = np.load(dataset_dir + 'train_y_arr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Group Normalization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNormalization(Layer):\n",
    "    def __init__(self, groups = 32, epsilon = 1e-5, **kwargs):\n",
    "        super(GroupNormalization, self).__init__(**kwargs)\n",
    "        self.groups = groups\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name = 'gamma', shape = (input_shape[-1],), initializer = 'ones', trainable = True)\n",
    "        self.beta = self.add_weight(name = 'beta', shape = (input_shape[-1],), initializer = 'zeros', trainable = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        group_shape = [input_shape[i] for i in range(len(input_shape) - 1)]\n",
    "        group_shape.append(input_shape[-1] // self.groups)\n",
    "        group_shape.append(self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        \n",
    "        x = tf.reshape(inputs, group_shape)\n",
    "        mean, variance = tf.nn.moments(x, [1, 2], keepdims=True)\n",
    "        x = (x - mean) / tf.sqrt(variance + self.epsilon)\n",
    "        x = tf.reshape(x, input_shape)\n",
    "        \n",
    "        return self.gamma * x + self.beta\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(GroupNormalization, self).get_config()\n",
    "        config.update({'groups': self.groups, 'epsilon': self.epsilon})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MT-ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input_Stream(Model):\n",
    "    def __init__(self, num_maps, kernel_size, pool):\n",
    "        super(Input_Stream, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2D(num_maps, kernel_size, strides = 2, use_bias = False)\n",
    "        self.bn1 = GroupNormalization()\n",
    "        self.conv2 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
    "        self.bn2 = GroupNormalization()\n",
    "        self.conv3 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
    "        self.bn3 = GroupNormalization()\n",
    "        self.relu = ReLU()\n",
    "        self.max_pool = MaxPool2D(pool_size = pool, strides = 2)\n",
    "        \n",
    "       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class ResBottleneckBlock(Model):\n",
    "    def __init__(self, filters, downsample):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.filters = filters\n",
    "        self.conv1 = Conv2D(filters, 1, 1)\n",
    "        if downsample:\n",
    "            self.conv2 = Conv2D(filters, 3, 2, padding = 'same')\n",
    "        else:\n",
    "            self.conv2 = Conv2D(filters, 3, 1, padding = 'same')\n",
    "        self.conv3 = Conv2D(filters * 4, 1, 1)\n",
    "        self.gn1 = GroupNormalization()\n",
    "        self.gn2 = GroupNormalization()\n",
    "        self.gn3 = GroupNormalization()\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        if self.downsample or self.filters * 4 != input_shape[-1]:\n",
    "            self.shortcut = Sequential([\n",
    "                Conv2D(\n",
    "                    self.filters * 4, 1, 2 if self.downsample else 1, padding = 'same'),\n",
    "                GroupNormalization()\n",
    "            ])\n",
    "        else:\n",
    "            self.shortcut = Sequential()\n",
    "\n",
    "            \n",
    "            \n",
    "    def call(self, input):\n",
    "        shortcut = self.shortcut(input)\n",
    "\n",
    "        input = self.conv1(input)\n",
    "        input = self.gn1(input)\n",
    "        input = ReLU()(input)\n",
    "\n",
    "        input = self.conv2(input)\n",
    "        input = self.gn2(input)\n",
    "        input = ReLU()(input)\n",
    "\n",
    "        input = self.conv3(input)\n",
    "        input = self.gn3(input)\n",
    "        input = ReLU()(input)\n",
    "\n",
    "        input = input + shortcut\n",
    "        return ReLU()(input)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ResNet(Model):\n",
    "    def __init__(self, repeat):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        self.layer0 = Sequential([\n",
    "            Conv2D(64, 7, 2, padding='same'),\n",
    "            MaxPool2D(pool_size = 3, strides = 2, padding = 'same'),\n",
    "            GroupNormalization(),\n",
    "            ReLU()\n",
    "        ], name = 'layer0')\n",
    "        '''\n",
    "        \n",
    "        self.input_conv = Input_Stream(num_maps = 64, kernel_size = 3, pool = 3)\n",
    "\n",
    "        self.layer1 = Sequential([\n",
    "            ResBottleneckBlock(64, downsample = False) for _ in range(repeat[0])\n",
    "        ], name = 'layer1')\n",
    "\n",
    "        self.layer2 = Sequential([\n",
    "            ResBottleneckBlock(128, downsample = True)\n",
    "        ] + [\n",
    "            ResBottleneckBlock(128, downsample = False) for _ in range(1, repeat[1])\n",
    "        ], name = 'layer2')\n",
    "\n",
    "        self.layer3 = Sequential([\n",
    "            ResBottleneckBlock(256, downsample = True)\n",
    "        ] + [\n",
    "            ResBottleneckBlock(256, downsample = False) for _ in range(1, repeat[2])\n",
    "        ], name = 'layer3')\n",
    "\n",
    "        self.layer4 = Sequential([\n",
    "            ResBottleneckBlock(512, downsample = True)\n",
    "        ] + [\n",
    "            ResBottleneckBlock(512, downsample = False) for _ in range(1, repeat[3])\n",
    "        ], name = 'layer4')\n",
    "\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.fc1 = Dense(1, name = 'fap', use_bias = False)\n",
    "        self.fc2 = Dense(1, activation = 'sigmoid', name = 'gender', use_bias = False)\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.input_conv(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.gap(x)\n",
    "        x_fap = self.fc1(x)\n",
    "        x_gender = self.fc2(x)\n",
    "        \n",
    "        return x_fap, x_gender\n",
    "    \n",
    "    \n",
    "\n",
    "class MT_ResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__([3, 4, 6, 3])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def call(self, input):\n",
    "        return super().call(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Loss Function Definition - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_Functions = {'output_1': 'mse', 'output_2': 'binary_crossentropy'}\n",
    "Loss_Weights = {'output_1': 2, 'output_2': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Callbacks Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.01) \n",
    "\n",
    "    \n",
    "    \n",
    "required_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(custom_scheduler),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5, verbose = 1, cooldown = 2, min_lr = 0.0000001),\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training Parameters Definition - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = Adam(learning_rate = 0.005)\n",
    "model = MT_ResNet()\n",
    "model.compile(optimizer = adam_optimizer, loss = Loss_Functions, loss_weights = Loss_Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Phase - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "550/550 [==============================] - 115s 176ms/step - loss: 2.9734 - output_1_loss: 1.0229 - output_2_loss: 0.9277 - val_loss: 1.6601 - val_output_1_loss: 0.4149 - val_output_2_loss: 0.8303 - lr: 0.0050\n",
      "Epoch 2/1000\n",
      "550/550 [==============================] - 93s 169ms/step - loss: 1.9736 - output_1_loss: 0.6220 - output_2_loss: 0.7296 - val_loss: 3.0334 - val_output_1_loss: 1.2124 - val_output_2_loss: 0.6087 - lr: 0.0050\n",
      "Epoch 3/1000\n",
      "550/550 [==============================] - 94s 170ms/step - loss: 2.1577 - output_1_loss: 0.7076 - output_2_loss: 0.7425 - val_loss: 2.4908 - val_output_1_loss: 0.8666 - val_output_2_loss: 0.7576 - lr: 0.0050\n",
      "Epoch 4/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 1.7461 - output_1_loss: 0.5405 - output_2_loss: 0.6652 - val_loss: 1.3919 - val_output_1_loss: 0.4149 - val_output_2_loss: 0.5620 - lr: 0.0050\n",
      "Epoch 5/1000\n",
      "550/550 [==============================] - 94s 171ms/step - loss: 1.7399 - output_1_loss: 0.5508 - output_2_loss: 0.6383 - val_loss: 1.8829 - val_output_1_loss: 0.6197 - val_output_2_loss: 0.6435 - lr: 0.0050\n",
      "Epoch 6/1000\n",
      "550/550 [==============================] - 93s 170ms/step - loss: 1.8674 - output_1_loss: 0.5900 - output_2_loss: 0.6873 - val_loss: 1.8524 - val_output_1_loss: 0.6253 - val_output_2_loss: 0.6018 - lr: 0.0050\n",
      "Epoch 7/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 1.7047 - output_1_loss: 0.5348 - output_2_loss: 0.6351 - val_loss: 1.5872 - val_output_1_loss: 0.5091 - val_output_2_loss: 0.5689 - lr: 0.0049\n",
      "Epoch 8/1000\n",
      "550/550 [==============================] - 98s 178ms/step - loss: 1.6557 - output_1_loss: 0.5176 - output_2_loss: 0.6205 - val_loss: 1.5606 - val_output_1_loss: 0.4663 - val_output_2_loss: 0.6280 - lr: 0.0049\n",
      "Epoch 9/1000\n",
      "550/550 [==============================] - ETA: 0s - loss: 1.7743 - output_1_loss: 0.5487 - output_2_loss: 0.6770\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.002401973819360137.\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 1.7743 - output_1_loss: 0.5487 - output_2_loss: 0.6770 - val_loss: 1.6143 - val_output_1_loss: 0.4851 - val_output_2_loss: 0.6440 - lr: 0.0048\n",
      "Epoch 10/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.5982 - output_1_loss: 0.4873 - output_2_loss: 0.6235 - val_loss: 1.4893 - val_output_1_loss: 0.4620 - val_output_2_loss: 0.5654 - lr: 0.0024\n",
      "Epoch 11/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.5527 - output_1_loss: 0.4759 - output_2_loss: 0.6009 - val_loss: 1.4469 - val_output_1_loss: 0.4328 - val_output_2_loss: 0.5812 - lr: 0.0024\n",
      "Epoch 12/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.4908 - output_1_loss: 0.4582 - output_2_loss: 0.5744 - val_loss: 1.3629 - val_output_1_loss: 0.3993 - val_output_2_loss: 0.5642 - lr: 0.0023\n",
      "Epoch 13/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.4080 - output_1_loss: 0.4405 - output_2_loss: 0.5269 - val_loss: 1.5624 - val_output_1_loss: 0.5231 - val_output_2_loss: 0.5162 - lr: 0.0023\n",
      "Epoch 14/1000\n",
      "550/550 [==============================] - 94s 172ms/step - loss: 1.4160 - output_1_loss: 0.4432 - output_2_loss: 0.5296 - val_loss: 1.3744 - val_output_1_loss: 0.4290 - val_output_2_loss: 0.5163 - lr: 0.0023\n",
      "Epoch 15/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.3824 - output_1_loss: 0.4389 - output_2_loss: 0.5047 - val_loss: 1.3878 - val_output_1_loss: 0.3998 - val_output_2_loss: 0.5881 - lr: 0.0023\n",
      "Epoch 16/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.3017 - output_1_loss: 0.4085 - output_2_loss: 0.4847 - val_loss: 1.4829 - val_output_1_loss: 0.5037 - val_output_2_loss: 0.4754 - lr: 0.0022\n",
      "Epoch 17/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 1.3389 - output_1_loss: 0.4160 - output_2_loss: 0.5069 - val_loss: 1.2580 - val_output_1_loss: 0.3672 - val_output_2_loss: 0.5235 - lr: 0.0022\n",
      "Epoch 18/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.2440 - output_1_loss: 0.3894 - output_2_loss: 0.4652 - val_loss: 1.0854 - val_output_1_loss: 0.3076 - val_output_2_loss: 0.4702 - lr: 0.0022\n",
      "Epoch 19/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 1.1153 - output_1_loss: 0.3386 - output_2_loss: 0.4382 - val_loss: 1.0631 - val_output_1_loss: 0.3229 - val_output_2_loss: 0.4174 - lr: 0.0022\n",
      "Epoch 20/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.0732 - output_1_loss: 0.3181 - output_2_loss: 0.4370 - val_loss: 1.0163 - val_output_1_loss: 0.2670 - val_output_2_loss: 0.4823 - lr: 0.0022\n",
      "Epoch 21/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.0563 - output_1_loss: 0.3099 - output_2_loss: 0.4364 - val_loss: 0.9599 - val_output_1_loss: 0.2628 - val_output_2_loss: 0.4342 - lr: 0.0021\n",
      "Epoch 22/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 1.0126 - output_1_loss: 0.2941 - output_2_loss: 0.4243 - val_loss: 0.8894 - val_output_1_loss: 0.2454 - val_output_2_loss: 0.3987 - lr: 0.0021\n",
      "Epoch 23/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.9691 - output_1_loss: 0.2819 - output_2_loss: 0.4052 - val_loss: 0.8658 - val_output_1_loss: 0.2398 - val_output_2_loss: 0.3861 - lr: 0.0021\n",
      "Epoch 24/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.9000 - output_1_loss: 0.2606 - output_2_loss: 0.3789 - val_loss: 0.9987 - val_output_1_loss: 0.3194 - val_output_2_loss: 0.3600 - lr: 0.0021\n",
      "Epoch 25/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.8624 - output_1_loss: 0.2518 - output_2_loss: 0.3588 - val_loss: 0.7882 - val_output_1_loss: 0.2219 - val_output_2_loss: 0.3444 - lr: 0.0020\n",
      "Epoch 26/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.8291 - output_1_loss: 0.2439 - output_2_loss: 0.3413 - val_loss: 0.8122 - val_output_1_loss: 0.2255 - val_output_2_loss: 0.3613 - lr: 0.0020\n",
      "Epoch 27/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.7644 - output_1_loss: 0.2273 - output_2_loss: 0.3099 - val_loss: 0.9106 - val_output_1_loss: 0.2761 - val_output_2_loss: 0.3584 - lr: 0.0020\n",
      "Epoch 28/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.7560 - output_1_loss: 0.2265 - output_2_loss: 0.3030 - val_loss: 0.7286 - val_output_1_loss: 0.2115 - val_output_2_loss: 0.3056 - lr: 0.0020\n",
      "Epoch 29/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.7016 - output_1_loss: 0.2144 - output_2_loss: 0.2728 - val_loss: 0.8579 - val_output_1_loss: 0.2501 - val_output_2_loss: 0.3576 - lr: 0.0020\n",
      "Epoch 30/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.6618 - output_1_loss: 0.2096 - output_2_loss: 0.2427 - val_loss: 0.7236 - val_output_1_loss: 0.2286 - val_output_2_loss: 0.2664 - lr: 0.0019\n",
      "Epoch 31/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.7211 - output_1_loss: 0.2197 - output_2_loss: 0.2817 - val_loss: 0.7352 - val_output_1_loss: 0.2166 - val_output_2_loss: 0.3020 - lr: 0.0019\n",
      "Epoch 32/1000\n",
      "550/550 [==============================] - 94s 171ms/step - loss: 0.7139 - output_1_loss: 0.2199 - output_2_loss: 0.2742 - val_loss: 0.9017 - val_output_1_loss: 0.2604 - val_output_2_loss: 0.3810 - lr: 0.0019\n",
      "Epoch 33/1000\n",
      "550/550 [==============================] - 94s 172ms/step - loss: 0.6779 - output_1_loss: 0.2129 - output_2_loss: 0.2521 - val_loss: 0.6751 - val_output_1_loss: 0.1998 - val_output_2_loss: 0.2754 - lr: 0.0019\n",
      "Epoch 34/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.5821 - output_1_loss: 0.1869 - output_2_loss: 0.2084 - val_loss: 0.7053 - val_output_1_loss: 0.2167 - val_output_2_loss: 0.2718 - lr: 0.0019\n",
      "Epoch 35/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.5236 - output_1_loss: 0.1740 - output_2_loss: 0.1756 - val_loss: 0.6741 - val_output_1_loss: 0.2100 - val_output_2_loss: 0.2541 - lr: 0.0019\n",
      "Epoch 36/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.5152 - output_1_loss: 0.1719 - output_2_loss: 0.1715 - val_loss: 0.7015 - val_output_1_loss: 0.2107 - val_output_2_loss: 0.2800 - lr: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.4611 - output_1_loss: 0.1629 - output_2_loss: 0.1353 - val_loss: 0.6002 - val_output_1_loss: 0.1976 - val_output_2_loss: 0.2049 - lr: 0.0018\n",
      "Epoch 38/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.4234 - output_1_loss: 0.1530 - output_2_loss: 0.1175 - val_loss: 0.6303 - val_output_1_loss: 0.2117 - val_output_2_loss: 0.2068 - lr: 0.0018\n",
      "Epoch 39/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.3645 - output_1_loss: 0.1385 - output_2_loss: 0.0875 - val_loss: 0.6584 - val_output_1_loss: 0.2229 - val_output_2_loss: 0.2126 - lr: 0.0018\n",
      "Epoch 40/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.3368 - output_1_loss: 0.1319 - output_2_loss: 0.0731 - val_loss: 0.6521 - val_output_1_loss: 0.1964 - val_output_2_loss: 0.2593 - lr: 0.0018\n",
      "Epoch 41/1000\n",
      "550/550 [==============================] - 94s 172ms/step - loss: 0.3258 - output_1_loss: 0.1231 - output_2_loss: 0.0796 - val_loss: 0.7166 - val_output_1_loss: 0.2001 - val_output_2_loss: 0.3164 - lr: 0.0017\n",
      "Epoch 42/1000\n",
      "550/550 [==============================] - ETA: 0s - loss: 0.2575 - output_1_loss: 0.1048 - output_2_loss: 0.0479\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0008634183323010802.\n",
      "550/550 [==============================] - 94s 172ms/step - loss: 0.2575 - output_1_loss: 0.1048 - output_2_loss: 0.0479 - val_loss: 0.6038 - val_output_1_loss: 0.2100 - val_output_2_loss: 0.1838 - lr: 0.0017\n",
      "Epoch 43/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.1751 - output_1_loss: 0.0765 - output_2_loss: 0.0221 - val_loss: 0.5623 - val_output_1_loss: 0.1938 - val_output_2_loss: 0.1748 - lr: 8.5483e-04\n",
      "Epoch 44/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.1428 - output_1_loss: 0.0639 - output_2_loss: 0.0151 - val_loss: 0.5768 - val_output_1_loss: 0.1901 - val_output_2_loss: 0.1967 - lr: 8.4632e-04\n",
      "Epoch 45/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.1203 - output_1_loss: 0.0545 - output_2_loss: 0.0114 - val_loss: 0.5785 - val_output_1_loss: 0.1927 - val_output_2_loss: 0.1932 - lr: 8.3790e-04\n",
      "Epoch 46/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.1018 - output_1_loss: 0.0466 - output_2_loss: 0.0085 - val_loss: 0.5769 - val_output_1_loss: 0.1968 - val_output_2_loss: 0.1833 - lr: 8.2956e-04\n",
      "Epoch 47/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.0875 - output_1_loss: 0.0401 - output_2_loss: 0.0074 - val_loss: 0.5869 - val_output_1_loss: 0.2003 - val_output_2_loss: 0.1863 - lr: 8.2131e-04\n",
      "Epoch 48/1000\n",
      "550/550 [==============================] - ETA: 0s - loss: 0.0799 - output_1_loss: 0.0365 - output_2_loss: 0.0068\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0004065683751832694.\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.0799 - output_1_loss: 0.0365 - output_2_loss: 0.0068 - val_loss: 0.6220 - val_output_1_loss: 0.2138 - val_output_2_loss: 0.1945 - lr: 8.1314e-04\n",
      "Epoch 49/1000\n",
      "550/550 [==============================] - 96s 174ms/step - loss: 0.0456 - output_1_loss: 0.0209 - output_2_loss: 0.0038 - val_loss: 0.6048 - val_output_1_loss: 0.2033 - val_output_2_loss: 0.1981 - lr: 4.0252e-04\n",
      "Epoch 50/1000\n",
      "550/550 [==============================] - 96s 175ms/step - loss: 0.0357 - output_1_loss: 0.0165 - output_2_loss: 0.0027 - val_loss: 0.6071 - val_output_1_loss: 0.2025 - val_output_2_loss: 0.2021 - lr: 3.9852e-04\n",
      "Epoch 51/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.0303 - output_1_loss: 0.0139 - output_2_loss: 0.0025 - val_loss: 0.6115 - val_output_1_loss: 0.2069 - val_output_2_loss: 0.1978 - lr: 3.9455e-04\n",
      "Epoch 52/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.0243 - output_1_loss: 0.0111 - output_2_loss: 0.0021 - val_loss: 0.6110 - val_output_1_loss: 0.2055 - val_output_2_loss: 0.2001 - lr: 3.9063e-04\n",
      "Epoch 53/1000\n",
      "550/550 [==============================] - 95s 172ms/step - loss: 0.0199 - output_1_loss: 0.0090 - output_2_loss: 0.0018 - val_loss: 0.6204 - val_output_1_loss: 0.2067 - val_output_2_loss: 0.2070 - lr: 3.8674e-04\n",
      "Epoch 54/1000\n",
      "550/550 [==============================] - ETA: 0s - loss: 0.0173 - output_1_loss: 0.0078 - output_2_loss: 0.0017\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.00019144582620356232.\n",
      "550/550 [==============================] - 96s 174ms/step - loss: 0.0173 - output_1_loss: 0.0078 - output_2_loss: 0.0017 - val_loss: 0.6196 - val_output_1_loss: 0.2071 - val_output_2_loss: 0.2054 - lr: 3.8289e-04\n",
      "Epoch 55/1000\n",
      "550/550 [==============================] - 96s 174ms/step - loss: 0.0108 - output_1_loss: 0.0048 - output_2_loss: 0.0012 - val_loss: 0.6207 - val_output_1_loss: 0.2060 - val_output_2_loss: 0.2087 - lr: 1.8954e-04\n",
      "Epoch 56/1000\n",
      "550/550 [==============================] - 95s 173ms/step - loss: 0.0081 - output_1_loss: 0.0035 - output_2_loss: 0.0011 - val_loss: 0.6247 - val_output_1_loss: 0.2069 - val_output_2_loss: 0.2110 - lr: 1.8765e-04\n",
      "Epoch 57/1000\n",
      "550/550 [==============================] - 96s 174ms/step - loss: 0.0067 - output_1_loss: 0.0029 - output_2_loss: 0.0010 - val_loss: 0.6262 - val_output_1_loss: 0.2082 - val_output_2_loss: 0.2097 - lr: 1.8579e-04\n",
      "Epoch 58/1000\n",
      "550/550 [==============================] - 96s 174ms/step - loss: 0.0055 - output_1_loss: 0.0023 - output_2_loss: 9.0506e-04 - val_loss: 0.6317 - val_output_1_loss: 0.2079 - val_output_2_loss: 0.2158 - lr: 1.8394e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x180ab3efac0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_img_arr, [train_y_arr[:, 0], train_y_arr[:, 1]], batch_size = 8, \n",
    "          epochs = 1000, verbose = 1, callbacks = required_callbacks, \n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('MT_ResNet_GN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SCUT-FBP Test Dataset Preparation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "rating_df = pd.read_excel(scut_dir + 'Rating_Collection\\\\Attractiveness label.xlsx', header = 0)\n",
    "img_arr = np.zeros([len(rating_df), 224, 224, 3])\n",
    "y_arr = np.zeros([len(rating_df), 2])\n",
    "\n",
    "for i in tqdm(range(len(rating_df))):\n",
    "    num = rating_df.iloc[i, 0]\n",
    "    img = cv2.imread(scut_dir + 'Data_Collection\\\\SCUT-FBP-' + str(num) + '.jpg')\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "        results = selfie_segmentation.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        condition = np.stack((results.segmentation_mask, ) * 3, axis = -1) > 0.1\n",
    "        bg_image = np.zeros(img.shape, dtype = np.uint8)\n",
    "        bg_image[:] = (255, 255, 255)\n",
    "        cleaned_img = np.where(condition, img, bg_image)\n",
    "    \n",
    "    cleaned_img = tf.keras.layers.Resizing(224, 224)(cleaned_img)\n",
    "    cleaned_img /= 255\n",
    "    img_arr[i] = cleaned_img\n",
    "    \n",
    "    y_arr[i, 0] = rating_df.iloc[i, 1]\n",
    "    y_arr[i, 1] = 1\n",
    "    \n",
    "np.save(scut_dir + 'img_arr.npy', img_arr)\n",
    "np.save(scut_dir + 'y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.load(scut_dir + 'img_arr.npy')\n",
    "y_arr = np.load(scut_dir + 'y_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 139ms/step - loss: 1.0062 - output_1_loss: 0.2866 - output_2_loss: 0.4330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0061659812927246, 0.28656190633773804, 0.4330422282218933]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(img_arr, [y_arr[:, 0], y_arr[:, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MEBeauty Test Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "rating_df = pd.read_csv(mebeauty_dir + 'landmarks.csv', header = 0)\n",
    "#img_arr = np.zeros([len(rating_df), 224, 224, 3])\n",
    "#y_arr = np.zeros([len(rating_df), 2])\n",
    "\n",
    "img_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in tqdm(range(len(rating_df))):\n",
    "    file_path = rating_df.iloc[i, 1]\n",
    "    file_name = '\\\\'.join(file_path.split('/')[5:])\n",
    "    gender = file_path.split('/')[5]\n",
    "        \n",
    "    try:\n",
    "        img = cv2.imread(mebeauty_dir + 'cropped_images\\\\images_crop_align_mtcnn\\\\' + file_name)\n",
    "        '''\n",
    "        with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "            results = selfie_segmentation.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            condition = np.stack((results.segmentation_mask, ) * 3, axis = -1) > 0.1\n",
    "            bg_image = np.zeros(img.shape, dtype = np.uint8)\n",
    "            bg_image[:] = (255, 255, 255)\n",
    "            cleaned_img = np.where(condition, img, bg_image)\n",
    "        '''\n",
    "\n",
    "        img = tf.keras.layers.Resizing(224, 224)(img)\n",
    "        img /= 255\n",
    "        #img_arr[i] = img\n",
    "        img = img[np.newaxis, :, :, :]\n",
    "        img_list.append(img)\n",
    "        \n",
    "        if gender == 'female':\n",
    "            #y_arr[i, 1] = 1\n",
    "            y_list.append(np.array([rating_df.iloc[i, 2] / 2, 1]).reshape(1, 2))\n",
    "        else:\n",
    "            #y_arr[i, 1] = 0\n",
    "            y_list.append(np.array([rating_df.iloc[i, 2] / 2, 0]).reshape(1, 2))\n",
    "            \n",
    "        #y_arr[i, 0] = rating_df.iloc[i, 2]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "img_arr = np.concatenate(img_list, axis = 0)\n",
    "y_arr = np.concatenate(y_list, axis = 0)\n",
    "    \n",
    "np.save(mebeauty_dir + 'img_arr.npy', img_arr)\n",
    "np.save(mebeauty_dir + 'y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.load(mebeauty_dir + 'img_arr.npy')\n",
    "y_arr = np.load(mebeauty_dir + 'y_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 14s 138ms/step - loss: 4.2271 - output_1_loss: 1.2101 - output_2_loss: 1.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.227126598358154, 1.21011483669281, 1.806898832321167]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = MT_ResNet()\n",
    "new_model.build((None, 224, 224, 3))\n",
    "new_model.load_weights('MT_ResNet_GN.h5')\n",
    "new_model.compile(optimizer = adam_optimizer, loss = Loss_Functions, loss_weights = Loss_Weights)\n",
    "\n",
    "new_model.evaluate(img_arr, [y_arr[:, 0], y_arr[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

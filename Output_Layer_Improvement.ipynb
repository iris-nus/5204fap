{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dLfKRJ_M8xo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf \n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Input, Model, Sequential\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "#from tensorflow.keras.applications.convnext import ConvNeXtTiny\n",
        "#from tensorflow.keras.applications.efficientnet import EfficientNetB2, EfficientNetB3\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet201\n",
        "#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "#from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
        "#from tensorflow.keras.applications.resnet_rs import ResNetRS50\n",
        "#from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "#from tensorflow.keras.applications.vgg16 import VGG16\n",
        "#from tensorflow.keras.applications.vgg19 import VGG19\n",
        "#from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.layers import Add, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Layer, MaxPool2D, ReLU, Resizing\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqJU1G-NNRZG"
      },
      "outputs": [],
      "source": [
        "# paths\n",
        "proj_path = 'Documents'\n",
        "dataset_path = proj_path + '/SCUT-FBP'\n",
        "image_path = dataset_path + '/Cleaned_Images/'\n",
        "rate_path = dataset_path + '/Rating_Collection' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Y_TL5M-Zbd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "folder_path = image_path # replace with the actual folder path\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if \"(1)\" in filename:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeGaaZsRN02V",
        "outputId": "bbfd71c8-c905-48f2-8270-55fc13208162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0\n",
            "Num GPUs Available 1\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n",
        "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ_WEXjnN6QX",
        "outputId": "c8e3b404-223d-4e8c-bf44-697b47063ea3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 5500/5500 [00:33<00:00, 164.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# load data: y_arr = [rating, race, gender]\n",
        "def load_data(img_dir, label_dir):\n",
        "  all_ratings = pd.read_csv(label_dir, sep = ' ', header = None)\n",
        "  all_ratings.columns = ['img_path', 'rating']\n",
        "\n",
        "  img_arr = np.zeros([len(all_ratings), 224, 224, 3])\n",
        "  y_arr = np.zeros([len(all_ratings), 3])\n",
        "\n",
        "  for i in tqdm(range(len(all_ratings))):\n",
        "  #for i in tqdm(range(500)):\n",
        "    file_name = all_ratings.iloc[i, 0]\n",
        "    race = file_name[0]\n",
        "    gender = file_name[1]\n",
        "    if race == 'A':\n",
        "      y_arr[i, 1] = 0\n",
        "    else:\n",
        "      y_arr[i, 1] = 1\n",
        "\n",
        "    if gender == 'M':\n",
        "      y_arr[i, 2] = 0\n",
        "    else:\n",
        "      y_arr[i, 2] = 1 \n",
        "        \n",
        "    y_arr[i, 0] = all_ratings.iloc[i, 1]\n",
        "    \n",
        "    img = tf.io.read_file(img_dir + file_name)\n",
        "    img = tf.image.decode_jpeg(img, channels = 3)\n",
        "    img = tf.keras.layers.Resizing(224, 224)(img)\n",
        "    img = preprocess_input(img)\n",
        "    img_arr[i] = img\n",
        "\n",
        "  return img_arr, y_arr\n",
        "\n",
        "img_arr, y_arr = load_data(image_path, rate_path + '/All_labels.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vl6mk_Wbign"
      },
      "outputs": [],
      "source": [
        "img_full_train, img_test, y_full_train, y_test = train_test_split(img_arr, y_arr, stratify = y_arr[:, 2], test_size = 0.2, \n",
        "                                                                  random_state = 0)\n",
        "img_train, img_val, y_train, y_val = train_test_split(img_full_train, y_full_train, stratify = y_full_train[:, 2], \n",
        "                                                      test_size = 0.2, random_state = 0)\n",
        "\n",
        "rating_train = y_train[:, 0]\n",
        "rating_val = y_val[:, 0]\n",
        "rating_test = y_test[:, 0]\n",
        "\n",
        "gender_train = y_train[:, 1]\n",
        "gender_val = y_val[:, 1]\n",
        "gender_test = y_test[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZmFVe9tbkuz"
      },
      "outputs": [],
      "source": [
        "class Input_Stream(Model):\n",
        "    def __init__(self, num_maps, kernel_size, momentum_parameter, pool):\n",
        "        super(Input_Stream, self).__init__()\n",
        "        \n",
        "        self.conv1 = Conv2D(num_maps, kernel_size, strides = 2, use_bias = False)\n",
        "        self.bn1 = BatchNormalization(momentum = momentum_parameter)\n",
        "        self.conv2 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
        "        self.bn2 = BatchNormalization(momentum = momentum_parameter)\n",
        "        self.conv3 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
        "        self.bn3 = BatchNormalization(momentum = momentum_parameter)\n",
        "        self.relu = ReLU()\n",
        "        self.max_pool = MaxPool2D(pool_size = pool, strides = 2)\n",
        "        \n",
        "       \n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.max_pool(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owp7YabRSLFe"
      },
      "outputs": [],
      "source": [
        "class Residual_Block(Model):\n",
        "    def __init__(self, num_maps, kernel_size, momentum_parameter):\n",
        "        super(Residual_Block, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2D(num_maps, kernel_size, activation='relu', padding='same')\n",
        "        self.bn1 = BatchNormalization(momentum = momentum_parameter)\n",
        "        self.conv2 = Conv2D(num_maps, kernel_size, padding='same')\n",
        "        self.add = Add()\n",
        "        self.relu = ReLU()\n",
        "        self.bn2 = BatchNormalization(momentum = momentum_parameter)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        fx = self.conv2(self.bn1(self.conv1(x)))\n",
        "        concat = self.add([x, fx])\n",
        "        out = self.bn2(self.relu(concat))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZkhebudb51Z"
      },
      "outputs": [],
      "source": [
        "class MT_ResNet(Model):\n",
        "    def __init__(self):\n",
        "        super(MT_ResNet, self).__init__()\n",
        "        \n",
        "        self.start_block = Input_Stream(num_maps = 64, kernel_size = 3, momentum_parameter = 0.5, pool = 3)\n",
        "        self.resnet50 = ResNet50(include_top = False, pooling = None)\n",
        "        self.resnet50_backbone = Model(self.resnet50.layers[7].input, self.resnet50.layers[-1].output)\n",
        "        self.res_block1 = Residual_Block(num_maps = 2048, kernel_size = 1, momentum_parameter = 0.5)\n",
        "        self.res_block2 = Residual_Block(num_maps = 2048, kernel_size = 1, momentum_parameter = 0.5)\n",
        "        self.global_pool1 = GlobalAveragePooling2D()\n",
        "        self.global_pool2 = GlobalAveragePooling2D()\n",
        "        self.fc1 = Dense(1, name = 'fap', use_bias = False)\n",
        "        self.fc2 = Dense(1, activation = 'sigmoid', name = 'gender', use_bias = False)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.start_block(inputs)\n",
        "        x = self.resnet50_backbone(x)\n",
        "        x_fap = self.global_pool1(self.res_block1(x))\n",
        "        x_fap = self.fc1(x_fap)\n",
        "        x_gender = self.global_pool2(self.res_block2(x))\n",
        "        x_gender = self.fc2(x_gender)\n",
        "        \n",
        "        return x_fap, x_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM28vU7vQj6A"
      },
      "outputs": [],
      "source": [
        "class MT_ResNet(Model):\n",
        "    def __init__(self):\n",
        "        super(MT_ResNet, self).__init__()\n",
        "        \n",
        "        self.start_block = Input_Stream(num_maps = 64, kernel_size = 3, momentum_parameter = 0.5, pool = 3)\n",
        "        self.resnet50 = ResNet50(include_top = False, pooling = 'avg')\n",
        "        self.resnet50_backbone = Model(self.resnet50.layers[7].input, self.resnet50.layers[-1].output)\n",
        "        \n",
        "        self.fap_block = Sequential([\n",
        "            Dense(128, activation='relu', use_bias=False),\n",
        "            BatchNormalization(momentum=0.5),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu', use_bias=False),\n",
        "            BatchNormalization(momentum=0.5),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, name = 'fap', use_bias = False)\n",
        "        ])\n",
        "\n",
        "        self.gender_block = Sequential([\n",
        "            Dense(8, activation='relu', use_bias=False),\n",
        "            BatchNormalization(momentum=0.5),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, activation = 'sigmoid', name = 'gender', use_bias = False)\n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.start_block(inputs)\n",
        "        x = self.resnet50_backbone(x)\n",
        "        x_fap = self.fap_block(x)\n",
        "        x_gender = self.gender_block(x)\n",
        "        \n",
        "        return x_fap, x_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWJVCeP6cArX"
      },
      "outputs": [],
      "source": [
        "Loss_Functions = {'output_1': 'mse', 'output_2': 'binary_crossentropy'}\n",
        "Loss_Weights = {'output_1': 2, 'output_2': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ2iaUFKcPOY"
      },
      "outputs": [],
      "source": [
        "adam_optimizer = Adam(learning_rate = 0.001)\n",
        "model = MT_ResNet()\n",
        "model.compile(optimizer = adam_optimizer, loss = Loss_Functions, loss_weights = Loss_Weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xPVkuTKdOTn"
      },
      "outputs": [],
      "source": [
        "def custom_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.01) \n",
        "\n",
        "    \n",
        "    \n",
        "required_callbacks = [\n",
        "    #tf.keras.callbacks.ModelCheckpoint(filepath = 'checkpoint', monitor = 'val_loss', save_best_only = True),\n",
        "    #tf.keras.callbacks.TensorBoard(),\n",
        "    tf.keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(custom_scheduler),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 10, verbose = 1, cooldown = 5, min_lr = 0.0000001),\n",
        "    tf.keras.callbacks.TerminateOnNaN()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a24ZQM8ndTUs",
        "outputId": "ee12a716-db78-4405-875a-c2fb4449392a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "1100/1100 [==============================] - 95s 75ms/step - loss: 5.6255 - output_1_loss: 2.5031 - output_2_loss: 0.6193 - val_loss: 7.9925 - val_output_1_loss: 3.6561 - val_output_2_loss: 0.6804 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "1100/1100 [==============================] - 80s 72ms/step - loss: 2.3259 - output_1_loss: 0.8656 - output_2_loss: 0.5948 - val_loss: 1.5704 - val_output_1_loss: 0.4591 - val_output_2_loss: 0.6522 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.9553 - output_1_loss: 0.6804 - output_2_loss: 0.5944 - val_loss: 1.3458 - val_output_1_loss: 0.3743 - val_output_2_loss: 0.5972 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.8851 - output_1_loss: 0.6452 - output_2_loss: 0.5948 - val_loss: 681469.0625 - val_output_1_loss: 340702.7500 - val_output_2_loss: 63.6854 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.7709 - output_1_loss: 0.6282 - output_2_loss: 0.5145 - val_loss: 301.5305 - val_output_1_loss: 150.4814 - val_output_2_loss: 0.5676 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.6932 - output_1_loss: 0.5952 - output_2_loss: 0.5028 - val_loss: 39969.5469 - val_output_1_loss: 19978.7129 - val_output_2_loss: 12.1406 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.7927 - output_1_loss: 0.6011 - output_2_loss: 0.5905 - val_loss: 3.2886 - val_output_1_loss: 1.3332 - val_output_2_loss: 0.6222 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.7553 - output_1_loss: 0.5837 - output_2_loss: 0.5879 - val_loss: 1.4993 - val_output_1_loss: 0.4677 - val_output_2_loss: 0.5640 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.7017 - output_1_loss: 0.5654 - output_2_loss: 0.5710 - val_loss: 5.0306 - val_output_1_loss: 2.2377 - val_output_2_loss: 0.5551 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "1100/1100 [==============================] - 80s 73ms/step - loss: 1.7173 - output_1_loss: 0.5683 - output_2_loss: 0.5807 - val_loss: 9.7012 - val_output_1_loss: 0.8995 - val_output_2_loss: 7.9022 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.7231 - output_1_loss: 0.5696 - output_2_loss: 0.5840 - val_loss: 97.1740 - val_output_1_loss: 48.3349 - val_output_2_loss: 0.5041 - lr: 9.9005e-04\n",
            "Epoch 12/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.6665 - output_1_loss: 0.5607 - output_2_loss: 0.5451 - val_loss: 15.0003 - val_output_1_loss: 7.2664 - val_output_2_loss: 0.4675 - lr: 9.8020e-04\n",
            "Epoch 13/1000\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 1.6036 - output_1_loss: 0.5504 - output_2_loss: 0.5028\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00048522278666496277.\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.6036 - output_1_loss: 0.5504 - output_2_loss: 0.5028 - val_loss: 1.5113 - val_output_1_loss: 0.5263 - val_output_2_loss: 0.4586 - lr: 9.7045e-04\n",
            "Epoch 14/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.6080 - output_1_loss: 0.5482 - output_2_loss: 0.5116 - val_loss: 1.5135 - val_output_1_loss: 0.5186 - val_output_2_loss: 0.4764 - lr: 4.8039e-04\n",
            "Epoch 15/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.5851 - output_1_loss: 0.5569 - output_2_loss: 0.4713 - val_loss: 1.3882 - val_output_1_loss: 0.4792 - val_output_2_loss: 0.4298 - lr: 4.7561e-04\n",
            "Epoch 16/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5456 - output_1_loss: 0.5468 - output_2_loss: 0.4520 - val_loss: 23.9776 - val_output_1_loss: 11.8091 - val_output_2_loss: 0.3593 - lr: 4.7088e-04\n",
            "Epoch 17/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.5415 - output_1_loss: 0.5462 - output_2_loss: 0.4492 - val_loss: 2.0611 - val_output_1_loss: 0.8464 - val_output_2_loss: 0.3684 - lr: 4.6620e-04\n",
            "Epoch 18/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.6238 - output_1_loss: 0.5429 - output_2_loss: 0.5380 - val_loss: 1.5606 - val_output_1_loss: 0.5049 - val_output_2_loss: 0.5507 - lr: 4.6156e-04\n",
            "Epoch 19/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5453 - output_1_loss: 0.5450 - output_2_loss: 0.4553 - val_loss: 171.9386 - val_output_1_loss: 85.8174 - val_output_2_loss: 0.3039 - lr: 4.5697e-04\n",
            "Epoch 20/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4922 - output_1_loss: 0.5418 - output_2_loss: 0.4086 - val_loss: 4.1584 - val_output_1_loss: 1.9427 - val_output_2_loss: 0.2730 - lr: 4.5242e-04\n",
            "Epoch 21/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4446 - output_1_loss: 0.5303 - output_2_loss: 0.3840 - val_loss: 93.6798 - val_output_1_loss: 46.7081 - val_output_2_loss: 0.2636 - lr: 4.4792e-04\n",
            "Epoch 22/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4537 - output_1_loss: 0.5248 - output_2_loss: 0.4042 - val_loss: 1.0748 - val_output_1_loss: 0.3880 - val_output_2_loss: 0.2987 - lr: 4.4346e-04\n",
            "Epoch 23/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5486 - output_1_loss: 0.5218 - output_2_loss: 0.5050 - val_loss: 1.0742 - val_output_1_loss: 0.3749 - val_output_2_loss: 0.3244 - lr: 4.3905e-04\n",
            "Epoch 24/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5328 - output_1_loss: 0.5174 - output_2_loss: 0.4981 - val_loss: 1.1722 - val_output_1_loss: 0.4112 - val_output_2_loss: 0.3497 - lr: 4.3468e-04\n",
            "Epoch 25/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5096 - output_1_loss: 0.5209 - output_2_loss: 0.4679 - val_loss: 1.6430 - val_output_1_loss: 0.6258 - val_output_2_loss: 0.3914 - lr: 4.3035e-04\n",
            "Epoch 26/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.5198 - output_1_loss: 0.5210 - output_2_loss: 0.4779 - val_loss: 22.3627 - val_output_1_loss: 11.0210 - val_output_2_loss: 0.3206 - lr: 4.2607e-04\n",
            "Epoch 27/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.5317 - output_1_loss: 0.5213 - output_2_loss: 0.4891 - val_loss: 46.5701 - val_output_1_loss: 23.1026 - val_output_2_loss: 0.3649 - lr: 4.2183e-04\n",
            "Epoch 28/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4238 - output_1_loss: 0.4896 - output_2_loss: 0.4446 - val_loss: 1.3219 - val_output_1_loss: 0.4731 - val_output_2_loss: 0.3758 - lr: 4.1764e-04\n",
            "Epoch 29/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4613 - output_1_loss: 0.4998 - output_2_loss: 0.4617 - val_loss: 50.5638 - val_output_1_loss: 25.1201 - val_output_2_loss: 0.3237 - lr: 4.1348e-04\n",
            "Epoch 30/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.3760 - output_1_loss: 0.4851 - output_2_loss: 0.4059 - val_loss: 820.9672 - val_output_1_loss: 410.3164 - val_output_2_loss: 0.3344 - lr: 4.0937e-04\n",
            "Epoch 31/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.3950 - output_1_loss: 0.4895 - output_2_loss: 0.4161 - val_loss: 1.9332 - val_output_1_loss: 0.6262 - val_output_2_loss: 0.6807 - lr: 4.0529e-04\n",
            "Epoch 32/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.5246 - output_1_loss: 0.5052 - output_2_loss: 0.5141 - val_loss: 32.1324 - val_output_1_loss: 15.8281 - val_output_2_loss: 0.4762 - lr: 4.0126e-04\n",
            "Epoch 33/1000\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 1.4151 - output_1_loss: 0.4890 - output_2_loss: 0.4371\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00019863341003656387.\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.4151 - output_1_loss: 0.4890 - output_2_loss: 0.4371 - val_loss: 1.1302 - val_output_1_loss: 0.4218 - val_output_2_loss: 0.2867 - lr: 3.9727e-04\n",
            "Epoch 34/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.3339 - output_1_loss: 0.4662 - output_2_loss: 0.4015 - val_loss: 1.2503 - val_output_1_loss: 0.5008 - val_output_2_loss: 0.2488 - lr: 1.9666e-04\n",
            "Epoch 35/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.3293 - output_1_loss: 0.4649 - output_2_loss: 0.3994 - val_loss: 18573.9199 - val_output_1_loss: 9286.6709 - val_output_2_loss: 0.5700 - lr: 1.9470e-04\n",
            "Epoch 36/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.3373 - output_1_loss: 0.4688 - output_2_loss: 0.3997 - val_loss: 14.7738 - val_output_1_loss: 7.2737 - val_output_2_loss: 0.2263 - lr: 1.9276e-04\n",
            "Epoch 37/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.2693 - output_1_loss: 0.4390 - output_2_loss: 0.3913 - val_loss: 319770.1875 - val_output_1_loss: 159884.8594 - val_output_2_loss: 0.4349 - lr: 1.9084e-04\n",
            "Epoch 38/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.2726 - output_1_loss: 0.4440 - output_2_loss: 0.3846 - val_loss: 1.3931 - val_output_1_loss: 0.4722 - val_output_2_loss: 0.4487 - lr: 1.8895e-04\n",
            "Epoch 39/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.3000 - output_1_loss: 0.4561 - output_2_loss: 0.3879 - val_loss: 10987620.0000 - val_output_1_loss: 5493809.5000 - val_output_2_loss: 0.6629 - lr: 1.8707e-04\n",
            "Epoch 40/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.2207 - output_1_loss: 0.4356 - output_2_loss: 0.3495 - val_loss: 2884492.7500 - val_output_1_loss: 1442246.1250 - val_output_2_loss: 0.3780 - lr: 1.8520e-04\n",
            "Epoch 41/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.1898 - output_1_loss: 0.4314 - output_2_loss: 0.3269 - val_loss: 14854.0400 - val_output_1_loss: 7426.8608 - val_output_2_loss: 0.3153 - lr: 1.8336e-04\n",
            "Epoch 42/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.2206 - output_1_loss: 0.4315 - output_2_loss: 0.3576 - val_loss: 70419.1484 - val_output_1_loss: 35209.4062 - val_output_2_loss: 0.3661 - lr: 1.8154e-04\n",
            "Epoch 43/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.2081 - output_1_loss: 0.4330 - output_2_loss: 0.3421 - val_loss: 13262.6250 - val_output_1_loss: 6631.1475 - val_output_2_loss: 0.3306 - lr: 1.7973e-04\n",
            "Epoch 44/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.1902 - output_1_loss: 0.4199 - output_2_loss: 0.3505 - val_loss: 49374.7109 - val_output_1_loss: 24687.1465 - val_output_2_loss: 0.4284 - lr: 1.7794e-04\n",
            "Epoch 45/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.1286 - output_1_loss: 0.4038 - output_2_loss: 0.3210 - val_loss: 145.5293 - val_output_1_loss: 72.5883 - val_output_2_loss: 0.3527 - lr: 1.7617e-04\n",
            "Epoch 46/1000\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.1573 - output_1_loss: 0.4192 - output_2_loss: 0.3188 - val_loss: 2964.9307 - val_output_1_loss: 1482.3297 - val_output_2_loss: 0.2726 - lr: 1.7442e-04\n",
            "Epoch 47/1000\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 1.1298 - output_1_loss: 0.4124 - output_2_loss: 0.3050\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 8.634179539512843e-05.\n",
            "1100/1100 [==============================] - 81s 74ms/step - loss: 1.1298 - output_1_loss: 0.4124 - output_2_loss: 0.3050 - val_loss: 5.6501 - val_output_1_loss: 2.6554 - val_output_2_loss: 0.3393 - lr: 1.7268e-04\n",
            "Epoch 48/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.1050 - output_1_loss: 0.3988 - output_2_loss: 0.3074 - val_loss: 42734.0000 - val_output_1_loss: 21366.6719 - val_output_2_loss: 0.6500 - lr: 8.5483e-05\n",
            "Epoch 49/1000\n",
            "1100/1100 [==============================] - 81s 73ms/step - loss: 1.0621 - output_1_loss: 0.3852 - output_2_loss: 0.2918 - val_loss: 16090.8857 - val_output_1_loss: 8045.3018 - val_output_2_loss: 0.2795 - lr: 8.4632e-05\n",
            "Epoch 50/1000\n",
            "1040/1100 [===========================>..] - ETA: 4s - loss: 1.0571 - output_1_loss: 0.3899 - output_2_loss: 0.2773"
          ]
        }
      ],
      "source": [
        "model.fit(img_arr, [y_arr[:, 0], y_arr[:, 1]], batch_size = 4, epochs = 1000, verbose = 1, \n",
        "          callbacks = required_callbacks, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi8zqF5lgOdC",
        "outputId": "8c42490f-71bc-4b83-d4c9-c027830144b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Documents/SCUT-FBP/saved_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Documents/SCUT-FBP/saved_model\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('Documents/SCUT-FBP/saved_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

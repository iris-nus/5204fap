{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c630a17b",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a96dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet201\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from tensorflow.keras.layers import Add, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Layer, MaxPool2D, ReLU, Resizing\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75ee5a",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac145b",
   "metadata": {},
   "source": [
    "## 2.1 Segment protrait\n",
    "Selfie segmentation model can segment the portrait of a person, and can be used for replacing or modifying the background in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df538f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "file_list = glob.glob(image_path + '/*.jpg')\n",
    "\n",
    "BG_COLOR = (255, 255, 255)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        file_name = file_list[i].split('/')[-1]\n",
    "        image = cv2.imread(file_list[i])\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "        \n",
    "        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        bg_image[:] = BG_COLOR\n",
    "\n",
    "        output_image = np.where(condition, image, bg_image)\n",
    "        \n",
    "        # cleaned images are under folder /DSA5204_FAP_Project/Datasets/SCUT-FBP/Cleaned_Images\n",
    "        cv2.imwrite(dataset_path + '/Cleaned_Images/' + file_name, output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f8ad8",
   "metadata": {},
   "source": [
    "## 2.2 Load cleaned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065fc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proj_path = '/content/drive/MyDrive/DSA5204_FAP_Project'\n",
    "#dataset_path = proj_path + '/Datasets/SCUT-FBP'\n",
    "image_path = '/users/xincai.zhang/Downloads/Cleaned_Images/'\n",
    "rate_path = '/users/xincai.zhang/Downloads/All_labels.txt'\n",
    "model_path =  './MT-Resnet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5841a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                | 0/5500 [00:00<?, ?it/s]2023-04-21 11:57:24.203123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5500/5500 [00:22<00:00, 245.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data: y_arr = [rating, race, gender]\n",
    "def load_data(img_dir, label_dir):\n",
    "    all_ratings = pd.read_csv(label_dir, sep = ' ', header = None)\n",
    "    all_ratings.columns = ['img_path', 'rating']\n",
    "\n",
    "    img_arr = np.zeros([len(all_ratings), 64, 64, 3])\n",
    "    y_arr = np.zeros([len(all_ratings), 3])\n",
    "\n",
    "    for i in tqdm(range(len(all_ratings))):\n",
    "    #for i in tqdm(range(500)):\n",
    "        file_name = all_ratings.iloc[i, 0]\n",
    "        race = file_name[0]\n",
    "        gender = file_name[1]\n",
    "        if race == 'A':\n",
    "            y_arr[i, 1] = 0\n",
    "        else:\n",
    "            y_arr[i, 1] = 1\n",
    "\n",
    "        if gender == 'M':\n",
    "            y_arr[i, 2] = 0\n",
    "        else:\n",
    "            y_arr[i, 2] = 1 \n",
    "        \n",
    "        y_arr[i, 0] = all_ratings.iloc[i, 1]\n",
    "    \n",
    "        img = tf.io.read_file(img_dir + file_name)\n",
    "        img = tf.image.decode_jpeg(img, channels = 3)\n",
    "        img = tf.keras.layers.Resizing(64, 64)(img)\n",
    "        img = preprocess_input(img)\n",
    "        img_arr[i] = img\n",
    "\n",
    "    return img_arr, y_arr\n",
    "\n",
    "img_arr, y_arr = load_data(image_path, rate_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8aa6f",
   "metadata": {},
   "source": [
    "## 2.3 Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6894abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_full_train, img_test, y_full_train, y_test = train_test_split(img_arr, y_arr, stratify = y_arr[:, 2], test_size = 0.4, \n",
    "                                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81adbf",
   "metadata": {},
   "source": [
    "# 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e00917",
   "metadata": {},
   "source": [
    "## 3.1 MT-Restnet model\n",
    "To adapt to FAP learning and multi-task requirement, the proposed model of MT-Resnet consists the following changes:\n",
    "- Input streaming layer is modified to three 3x3 convolution layers, instead of a 7x7 convolution layer, to reduce the training cost significantly. \n",
    "- Backbone of resnet50 consists of 4 stages, and each stage includes a  down-sampling block and several residual blocks based on the sequence of stages. \n",
    "- For the FAP task, the output part is implemented using a global average pooling layer followed by a fully connected layer. Linear activation function is utilized to predict facial attractiveness score.\n",
    "- For a gender prediction task, the structure of the output part is similar, whereas the activation function is sigmoid for the classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f2c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input_Stream(Model):\n",
    "    def __init__(self, num_maps, kernel_size, momentum_parameter, pool):\n",
    "        super(Input_Stream, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2D(num_maps, kernel_size, strides = 2, use_bias = False)\n",
    "        self.bn1 = BatchNormalization(momentum = momentum_parameter)\n",
    "        self.conv2 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
    "        self.bn2 = BatchNormalization(momentum = momentum_parameter)\n",
    "        self.conv3 = Conv2D(num_maps, kernel_size, padding = 'same', use_bias = False)\n",
    "        self.bn3 = BatchNormalization(momentum = momentum_parameter)\n",
    "        self.relu = ReLU()\n",
    "        self.max_pool = MaxPool2D(pool_size = pool, strides = 2)\n",
    "        \n",
    "       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MT_ResNet(Model):\n",
    "    def __init__(self):\n",
    "        super(MT_ResNet, self).__init__()\n",
    "        \n",
    "        self.start_block = Input_Stream(num_maps = 64, kernel_size = 3, momentum_parameter = 0.5, pool = 3)\n",
    "        self.resnet50 = ResNet50(include_top = False, pooling = 'avg')\n",
    "        self.resnet50_backbone = Model(self.resnet50.layers[7].input, self.resnet50.layers[-1].output)\n",
    "        self.fc1 = Dense(1, name = 'fap', use_bias = False)\n",
    "        self.fc2 = Dense(1, activation = 'sigmoid', name = 'gender', use_bias = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.start_block(inputs)\n",
    "        x = self.resnet50_backbone(x)\n",
    "        x_fap = self.fc1(x)\n",
    "        x_gender = self.fc2(x)\n",
    "        \n",
    "        return x_fap, x_gender\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15e462",
   "metadata": {},
   "source": [
    "## 3.2 Callbacks used to prevent overfitting\n",
    "- Early stopping\n",
    "- Learning rate scheduler\n",
    "- Reduce learning rate on plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7118b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.01) \n",
    "\n",
    "    \n",
    "    \n",
    "required_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(custom_scheduler),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 10, verbose = 1, cooldown = 5, min_lr = 0.0000001),\n",
    "    tf.keras.callbacks.TerminateOnNaN(),\n",
    "    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto'),\n",
    "    tf.keras.callbacks.CSVLogger('./MT-Resnet/model_history_log_1.csv', append=True)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351db52",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_train = False\n",
    "if continue_train:\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    model = MT_ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911ae01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "132/132 [==============================] - ETA: 0s - loss: 1035.0428 - output_1_loss: 516.9102 - output_2_loss: 1.2221 - output_1_mse: 516.9102 - output_2_accuracy: 0.5280\n",
      "Epoch 1: val_loss improved from inf to 1946561152.00000, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 142s 971ms/step - loss: 1035.0428 - output_1_loss: 516.9102 - output_2_loss: 1.2221 - output_1_mse: 516.9102 - output_2_accuracy: 0.5280 - val_loss: 1946561152.0000 - val_output_1_loss: 973274432.0000 - val_output_2_loss: 12165.3633 - val_output_1_mse: 973274432.0000 - val_output_2_accuracy: 0.5182 - lr: 0.1000\n",
      "Epoch 2/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 2.1022 - output_1_loss: 0.6407 - output_2_loss: 0.8208 - output_1_mse: 0.6407 - output_2_accuracy: 0.5697\n",
      "Epoch 2: val_loss improved from 1946561152.00000 to 371177.34375, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 126s 959ms/step - loss: 2.1022 - output_1_loss: 0.6407 - output_2_loss: 0.8208 - output_1_mse: 0.6407 - output_2_accuracy: 0.5697 - val_loss: 371177.3438 - val_output_1_loss: 185533.8906 - val_output_2_loss: 109.5806 - val_output_1_mse: 185533.8594 - val_output_2_accuracy: 0.5424 - lr: 0.1000\n",
      "Epoch 3/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 2.1171 - output_1_loss: 0.6323 - output_2_loss: 0.8526 - output_1_mse: 0.6323 - output_2_accuracy: 0.5841\n",
      "Epoch 3: val_loss improved from 371177.34375 to 390.06140, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 125s 950ms/step - loss: 2.1171 - output_1_loss: 0.6323 - output_2_loss: 0.8526 - output_1_mse: 0.6323 - output_2_accuracy: 0.5841 - val_loss: 390.0614 - val_output_1_loss: 193.9865 - val_output_2_loss: 2.0884 - val_output_1_mse: 193.9865 - val_output_2_accuracy: 0.6121 - lr: 0.1000\n",
      "Epoch 4/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 2.0451 - output_1_loss: 0.6227 - output_2_loss: 0.7998 - output_1_mse: 0.6227 - output_2_accuracy: 0.5947\n",
      "Epoch 4: val_loss improved from 390.06140 to 5.79508, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 128s 969ms/step - loss: 2.0451 - output_1_loss: 0.6227 - output_2_loss: 0.7998 - output_1_mse: 0.6227 - output_2_accuracy: 0.5947 - val_loss: 5.7951 - val_output_1_loss: 2.5136 - val_output_2_loss: 0.7679 - val_output_1_mse: 2.5136 - val_output_2_accuracy: 0.5530 - lr: 0.1000\n",
      "Epoch 5/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 2.1473 - output_1_loss: 0.7055 - output_2_loss: 0.7364 - output_1_mse: 0.7055 - output_2_accuracy: 0.5875\n",
      "Epoch 5: val_loss did not improve from 5.79508\n",
      "132/132 [==============================] - 105s 799ms/step - loss: 2.1473 - output_1_loss: 0.7055 - output_2_loss: 0.7364 - output_1_mse: 0.7055 - output_2_accuracy: 0.5875 - val_loss: 10.2034 - val_output_1_loss: 4.7745 - val_output_2_loss: 0.6545 - val_output_1_mse: 4.7745 - val_output_2_accuracy: 0.6030 - lr: 0.1000\n",
      "Epoch 6/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 2.1313 - output_1_loss: 0.6091 - output_2_loss: 0.9132 - output_1_mse: 0.6091 - output_2_accuracy: 0.5894\n",
      "Epoch 6: val_loss improved from 5.79508 to 1.90982, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 130s 988ms/step - loss: 2.1313 - output_1_loss: 0.6091 - output_2_loss: 0.9132 - output_1_mse: 0.6091 - output_2_accuracy: 0.5894 - val_loss: 1.9098 - val_output_1_loss: 0.6181 - val_output_2_loss: 0.6737 - val_output_1_mse: 0.6181 - val_output_2_accuracy: 0.6303 - lr: 0.1000\n",
      "Epoch 7/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.6917 - output_1_loss: 0.5027 - output_2_loss: 0.6863 - output_1_mse: 0.5027 - output_2_accuracy: 0.6231\n",
      "Epoch 7: val_loss improved from 1.90982 to 1.67447, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 127s 962ms/step - loss: 1.6917 - output_1_loss: 0.5027 - output_2_loss: 0.6863 - output_1_mse: 0.5027 - output_2_accuracy: 0.6231 - val_loss: 1.6745 - val_output_1_loss: 0.5191 - val_output_2_loss: 0.6363 - val_output_1_mse: 0.5191 - val_output_2_accuracy: 0.6530 - lr: 0.1000\n",
      "Epoch 8/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.6131 - output_1_loss: 0.4599 - output_2_loss: 0.6934 - output_1_mse: 0.4599 - output_2_accuracy: 0.6470\n",
      "Epoch 8: val_loss did not improve from 1.67447\n",
      "132/132 [==============================] - 110s 830ms/step - loss: 1.6131 - output_1_loss: 0.4599 - output_2_loss: 0.6934 - output_1_mse: 0.4599 - output_2_accuracy: 0.6470 - val_loss: 1.8149 - val_output_1_loss: 0.6164 - val_output_2_loss: 0.5821 - val_output_1_mse: 0.6164 - val_output_2_accuracy: 0.7091 - lr: 0.1000\n",
      "Epoch 9/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.5398 - output_1_loss: 0.4579 - output_2_loss: 0.6240 - output_1_mse: 0.4579 - output_2_accuracy: 0.6727\n",
      "Epoch 9: val_loss did not improve from 1.67447\n",
      "132/132 [==============================] - 112s 847ms/step - loss: 1.5398 - output_1_loss: 0.4579 - output_2_loss: 0.6240 - output_1_mse: 0.4579 - output_2_accuracy: 0.6727 - val_loss: 5.7108 - val_output_1_loss: 2.1864 - val_output_2_loss: 1.3380 - val_output_1_mse: 2.1864 - val_output_2_accuracy: 0.5485 - lr: 0.1000\n",
      "Epoch 10/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.6249 - output_1_loss: 0.4872 - output_2_loss: 0.6504 - output_1_mse: 0.4872 - output_2_accuracy: 0.6769\n",
      "Epoch 10: val_loss improved from 1.67447 to 1.41222, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 137s 1s/step - loss: 1.6249 - output_1_loss: 0.4872 - output_2_loss: 0.6504 - output_1_mse: 0.4872 - output_2_accuracy: 0.6769 - val_loss: 1.4122 - val_output_1_loss: 0.3992 - val_output_2_loss: 0.6138 - val_output_1_mse: 0.3992 - val_output_2_accuracy: 0.7015 - lr: 0.1000\n",
      "Epoch 11/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.5185 - output_1_loss: 0.4275 - output_2_loss: 0.6634 - output_1_mse: 0.4275 - output_2_accuracy: 0.6837\n",
      "Epoch 11: val_loss improved from 1.41222 to 1.30703, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 138s 1s/step - loss: 1.5185 - output_1_loss: 0.4275 - output_2_loss: 0.6634 - output_1_mse: 0.4275 - output_2_accuracy: 0.6837 - val_loss: 1.3070 - val_output_1_loss: 0.3588 - val_output_2_loss: 0.5893 - val_output_1_mse: 0.3588 - val_output_2_accuracy: 0.6909 - lr: 0.0990\n",
      "Epoch 12/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.3925 - output_1_loss: 0.4108 - output_2_loss: 0.5709 - output_1_mse: 0.4108 - output_2_accuracy: 0.7083\n",
      "Epoch 12: val_loss did not improve from 1.30703\n",
      "132/132 [==============================] - 110s 830ms/step - loss: 1.3925 - output_1_loss: 0.4108 - output_2_loss: 0.5709 - output_1_mse: 0.4108 - output_2_accuracy: 0.7083 - val_loss: 1.4432 - val_output_1_loss: 0.4225 - val_output_2_loss: 0.5981 - val_output_1_mse: 0.4225 - val_output_2_accuracy: 0.6742 - lr: 0.0980\n",
      "Epoch 13/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.3627 - output_1_loss: 0.3958 - output_2_loss: 0.5711 - output_1_mse: 0.3958 - output_2_accuracy: 0.7102\n",
      "Epoch 13: val_loss improved from 1.30703 to 1.24521, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 136s 1s/step - loss: 1.3627 - output_1_loss: 0.3958 - output_2_loss: 0.5711 - output_1_mse: 0.3958 - output_2_accuracy: 0.7102 - val_loss: 1.2452 - val_output_1_loss: 0.3638 - val_output_2_loss: 0.5176 - val_output_1_mse: 0.3638 - val_output_2_accuracy: 0.7364 - lr: 0.0970\n",
      "Epoch 14/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.4046 - output_1_loss: 0.4337 - output_2_loss: 0.5372 - output_1_mse: 0.4337 - output_2_accuracy: 0.7246\n",
      "Epoch 14: val_loss did not improve from 1.24521\n",
      "132/132 [==============================] - 111s 842ms/step - loss: 1.4046 - output_1_loss: 0.4337 - output_2_loss: 0.5372 - output_1_mse: 0.4337 - output_2_accuracy: 0.7246 - val_loss: 1.6008 - val_output_1_loss: 0.5420 - val_output_2_loss: 0.5167 - val_output_1_mse: 0.5420 - val_output_2_accuracy: 0.7576 - lr: 0.0961\n",
      "Epoch 15/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.3155 - output_1_loss: 0.4039 - output_2_loss: 0.5077 - output_1_mse: 0.4039 - output_2_accuracy: 0.7530\n",
      "Epoch 15: val_loss did not improve from 1.24521\n",
      "132/132 [==============================] - 112s 844ms/step - loss: 1.3155 - output_1_loss: 0.4039 - output_2_loss: 0.5077 - output_1_mse: 0.4039 - output_2_accuracy: 0.7530 - val_loss: 1.3203 - val_output_1_loss: 0.4141 - val_output_2_loss: 0.4921 - val_output_1_mse: 0.4141 - val_output_2_accuracy: 0.7379 - lr: 0.0951\n",
      "Epoch 16/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.2200 - output_1_loss: 0.3645 - output_2_loss: 0.4910 - output_1_mse: 0.3645 - output_2_accuracy: 0.7614\n",
      "Epoch 16: val_loss improved from 1.24521 to 1.18899, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 137s 1s/step - loss: 1.2200 - output_1_loss: 0.3645 - output_2_loss: 0.4910 - output_1_mse: 0.3645 - output_2_accuracy: 0.7614 - val_loss: 1.1890 - val_output_1_loss: 0.3033 - val_output_2_loss: 0.5825 - val_output_1_mse: 0.3033 - val_output_2_accuracy: 0.7136 - lr: 0.0942\n",
      "Epoch 17/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.1343 - output_1_loss: 0.3343 - output_2_loss: 0.4656 - output_1_mse: 0.3343 - output_2_accuracy: 0.7841\n",
      "Epoch 17: val_loss did not improve from 1.18899\n",
      "132/132 [==============================] - 113s 853ms/step - loss: 1.1343 - output_1_loss: 0.3343 - output_2_loss: 0.4656 - output_1_mse: 0.3343 - output_2_accuracy: 0.7841 - val_loss: 1.3169 - val_output_1_loss: 0.3820 - val_output_2_loss: 0.5528 - val_output_1_mse: 0.3820 - val_output_2_accuracy: 0.7288 - lr: 0.0932\n",
      "Epoch 18/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.1358 - output_1_loss: 0.3372 - output_2_loss: 0.4615 - output_1_mse: 0.3372 - output_2_accuracy: 0.7818\n",
      "Epoch 18: val_loss improved from 1.18899 to 1.08211, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 138s 1s/step - loss: 1.1358 - output_1_loss: 0.3372 - output_2_loss: 0.4615 - output_1_mse: 0.3372 - output_2_accuracy: 0.7818 - val_loss: 1.0821 - val_output_1_loss: 0.3308 - val_output_2_loss: 0.4206 - val_output_1_mse: 0.3308 - val_output_2_accuracy: 0.8182 - lr: 0.0923\n",
      "Epoch 19/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.0769 - output_1_loss: 0.3249 - output_2_loss: 0.4270 - output_1_mse: 0.3249 - output_2_accuracy: 0.8087\n",
      "Epoch 19: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 111s 843ms/step - loss: 1.0769 - output_1_loss: 0.3249 - output_2_loss: 0.4270 - output_1_mse: 0.3249 - output_2_accuracy: 0.8087 - val_loss: 1.1065 - val_output_1_loss: 0.3494 - val_output_2_loss: 0.4077 - val_output_1_mse: 0.3494 - val_output_2_accuracy: 0.8167 - lr: 0.0914\n",
      "Epoch 20/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.0606 - output_1_loss: 0.3291 - output_2_loss: 0.4024 - output_1_mse: 0.3291 - output_2_accuracy: 0.8208\n",
      "Epoch 20: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 112s 848ms/step - loss: 1.0606 - output_1_loss: 0.3291 - output_2_loss: 0.4024 - output_1_mse: 0.3291 - output_2_accuracy: 0.8208 - val_loss: 20.1995 - val_output_1_loss: 9.5677 - val_output_2_loss: 1.0642 - val_output_1_mse: 9.5677 - val_output_2_accuracy: 0.5818 - lr: 0.0905\n",
      "Epoch 21/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.0620 - output_1_loss: 0.3068 - output_2_loss: 0.4483 - output_1_mse: 0.3068 - output_2_accuracy: 0.7837\n",
      "Epoch 21: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 111s 844ms/step - loss: 1.0620 - output_1_loss: 0.3068 - output_2_loss: 0.4483 - output_1_mse: 0.3068 - output_2_accuracy: 0.7837 - val_loss: 1.2153 - val_output_1_loss: 0.3441 - val_output_2_loss: 0.5271 - val_output_1_mse: 0.3441 - val_output_2_accuracy: 0.7500 - lr: 0.0896\n",
      "Epoch 22/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.9711 - output_1_loss: 0.2966 - output_2_loss: 0.3779 - output_1_mse: 0.2966 - output_2_accuracy: 0.8364\n",
      "Epoch 22: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 112s 853ms/step - loss: 0.9711 - output_1_loss: 0.2966 - output_2_loss: 0.3779 - output_1_mse: 0.2966 - output_2_accuracy: 0.8364 - val_loss: 2.4512 - val_output_1_loss: 0.9985 - val_output_2_loss: 0.4541 - val_output_1_mse: 0.9985 - val_output_2_accuracy: 0.7985 - lr: 0.0887\n",
      "Epoch 23/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 1.0238 - output_1_loss: 0.3235 - output_2_loss: 0.3769 - output_1_mse: 0.3235 - output_2_accuracy: 0.8379\n",
      "Epoch 23: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 112s 847ms/step - loss: 1.0238 - output_1_loss: 0.3235 - output_2_loss: 0.3769 - output_1_mse: 0.3235 - output_2_accuracy: 0.8379 - val_loss: 1.3786 - val_output_1_loss: 0.5012 - val_output_2_loss: 0.3762 - val_output_1_mse: 0.5012 - val_output_2_accuracy: 0.8333 - lr: 0.0878\n",
      "Epoch 24/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.9134 - output_1_loss: 0.2838 - output_2_loss: 0.3459 - output_1_mse: 0.2838 - output_2_accuracy: 0.8519\n",
      "Epoch 24: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 112s 847ms/step - loss: 0.9134 - output_1_loss: 0.2838 - output_2_loss: 0.3459 - output_1_mse: 0.2838 - output_2_accuracy: 0.8519 - val_loss: 6.1205 - val_output_1_loss: 2.8435 - val_output_2_loss: 0.4336 - val_output_1_mse: 2.8435 - val_output_2_accuracy: 0.8379 - lr: 0.0869\n",
      "Epoch 25/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8487 - output_1_loss: 0.2635 - output_2_loss: 0.3217 - output_1_mse: 0.2635 - output_2_accuracy: 0.8530\n",
      "Epoch 25: val_loss did not improve from 1.08211\n",
      "132/132 [==============================] - 112s 846ms/step - loss: 0.8487 - output_1_loss: 0.2635 - output_2_loss: 0.3217 - output_1_mse: 0.2635 - output_2_accuracy: 0.8530 - val_loss: 1.1442 - val_output_1_loss: 0.3770 - val_output_2_loss: 0.3902 - val_output_1_mse: 0.3770 - val_output_2_accuracy: 0.8364 - lr: 0.0861\n",
      "Epoch 26/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8120 - output_1_loss: 0.2462 - output_2_loss: 0.3197 - output_1_mse: 0.2462 - output_2_accuracy: 0.8648\n",
      "Epoch 26: val_loss improved from 1.08211 to 0.86055, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 137s 1s/step - loss: 0.8120 - output_1_loss: 0.2462 - output_2_loss: 0.3197 - output_1_mse: 0.2462 - output_2_accuracy: 0.8648 - val_loss: 0.8606 - val_output_1_loss: 0.2743 - val_output_2_loss: 0.3120 - val_output_1_mse: 0.2743 - val_output_2_accuracy: 0.8652 - lr: 0.0852\n",
      "Epoch 27/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8044 - output_1_loss: 0.2545 - output_2_loss: 0.2954 - output_1_mse: 0.2545 - output_2_accuracy: 0.8777\n",
      "Epoch 27: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 103s 778ms/step - loss: 0.8044 - output_1_loss: 0.2545 - output_2_loss: 0.2954 - output_1_mse: 0.2545 - output_2_accuracy: 0.8777 - val_loss: 1.3605 - val_output_1_loss: 0.4629 - val_output_2_loss: 0.4347 - val_output_1_mse: 0.4629 - val_output_2_accuracy: 0.8242 - lr: 0.0844\n",
      "Epoch 28/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8530 - output_1_loss: 0.2839 - output_2_loss: 0.2851 - output_1_mse: 0.2839 - output_2_accuracy: 0.8864\n",
      "Epoch 28: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 102s 773ms/step - loss: 0.8530 - output_1_loss: 0.2839 - output_2_loss: 0.2851 - output_1_mse: 0.2839 - output_2_accuracy: 0.8864 - val_loss: 1.0066 - val_output_1_loss: 0.3256 - val_output_2_loss: 0.3553 - val_output_1_mse: 0.3256 - val_output_2_accuracy: 0.8500 - lr: 0.0835\n",
      "Epoch 29/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8231 - output_1_loss: 0.2636 - output_2_loss: 0.2959 - output_1_mse: 0.2636 - output_2_accuracy: 0.8780\n",
      "Epoch 29: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 102s 770ms/step - loss: 0.8231 - output_1_loss: 0.2636 - output_2_loss: 0.2959 - output_1_mse: 0.2636 - output_2_accuracy: 0.8780 - val_loss: 1.4823 - val_output_1_loss: 0.5478 - val_output_2_loss: 0.3866 - val_output_1_mse: 0.5478 - val_output_2_accuracy: 0.8318 - lr: 0.0827\n",
      "Epoch 30/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.8289 - output_1_loss: 0.2870 - output_2_loss: 0.2549 - output_1_mse: 0.2870 - output_2_accuracy: 0.9023\n",
      "Epoch 30: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 106s 804ms/step - loss: 0.8289 - output_1_loss: 0.2870 - output_2_loss: 0.2549 - output_1_mse: 0.2870 - output_2_accuracy: 0.9023 - val_loss: 2.3371 - val_output_1_loss: 0.4414 - val_output_2_loss: 1.4542 - val_output_1_mse: 0.4414 - val_output_2_accuracy: 0.6152 - lr: 0.0819\n",
      "Epoch 31/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.7419 - output_1_loss: 0.2386 - output_2_loss: 0.2647 - output_1_mse: 0.2386 - output_2_accuracy: 0.8909\n",
      "Epoch 31: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 102s 772ms/step - loss: 0.7419 - output_1_loss: 0.2386 - output_2_loss: 0.2647 - output_1_mse: 0.2386 - output_2_accuracy: 0.8909 - val_loss: 0.9312 - val_output_1_loss: 0.3203 - val_output_2_loss: 0.2907 - val_output_1_mse: 0.3203 - val_output_2_accuracy: 0.8712 - lr: 0.0811\n",
      "Epoch 32/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.6748 - output_1_loss: 0.2076 - output_2_loss: 0.2596 - output_1_mse: 0.2076 - output_2_accuracy: 0.8947\n",
      "Epoch 32: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 102s 773ms/step - loss: 0.6748 - output_1_loss: 0.2076 - output_2_loss: 0.2596 - output_1_mse: 0.2076 - output_2_accuracy: 0.8947 - val_loss: 0.9925 - val_output_1_loss: 0.3052 - val_output_2_loss: 0.3821 - val_output_1_mse: 0.3052 - val_output_2_accuracy: 0.8364 - lr: 0.0803\n",
      "Epoch 33/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.6783 - output_1_loss: 0.2213 - output_2_loss: 0.2358 - output_1_mse: 0.2213 - output_2_accuracy: 0.9068\n",
      "Epoch 33: val_loss did not improve from 0.86055\n",
      "132/132 [==============================] - 102s 773ms/step - loss: 0.6783 - output_1_loss: 0.2213 - output_2_loss: 0.2358 - output_1_mse: 0.2213 - output_2_accuracy: 0.9068 - val_loss: 1.1167 - val_output_1_loss: 0.4024 - val_output_2_loss: 0.3119 - val_output_1_mse: 0.4024 - val_output_2_accuracy: 0.8818 - lr: 0.0795\n",
      "Epoch 34/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.6045 - output_1_loss: 0.1967 - output_2_loss: 0.2112 - output_1_mse: 0.1967 - output_2_accuracy: 0.9178\n",
      "Epoch 34: val_loss improved from 0.86055 to 0.83374, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 126s 957ms/step - loss: 0.6045 - output_1_loss: 0.1967 - output_2_loss: 0.2112 - output_1_mse: 0.1967 - output_2_accuracy: 0.9178 - val_loss: 0.8337 - val_output_1_loss: 0.2809 - val_output_2_loss: 0.2720 - val_output_1_mse: 0.2809 - val_output_2_accuracy: 0.8939 - lr: 0.0787\n",
      "Epoch 35/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5643 - output_1_loss: 0.1937 - output_2_loss: 0.1769 - output_1_mse: 0.1937 - output_2_accuracy: 0.9356\n",
      "Epoch 35: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 103s 782ms/step - loss: 0.5643 - output_1_loss: 0.1937 - output_2_loss: 0.1769 - output_1_mse: 0.1937 - output_2_accuracy: 0.9356 - val_loss: 0.8548 - val_output_1_loss: 0.2759 - val_output_2_loss: 0.3031 - val_output_1_mse: 0.2759 - val_output_2_accuracy: 0.8818 - lr: 0.0779\n",
      "Epoch 36/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5400 - output_1_loss: 0.1694 - output_2_loss: 0.2012 - output_1_mse: 0.1694 - output_2_accuracy: 0.9167\n",
      "Epoch 36: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 110s 835ms/step - loss: 0.5400 - output_1_loss: 0.1694 - output_2_loss: 0.2012 - output_1_mse: 0.1694 - output_2_accuracy: 0.9167 - val_loss: 0.8578 - val_output_1_loss: 0.2709 - val_output_2_loss: 0.3160 - val_output_1_mse: 0.2709 - val_output_2_accuracy: 0.8773 - lr: 0.0771\n",
      "Epoch 37/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5487 - output_1_loss: 0.1835 - output_2_loss: 0.1818 - output_1_mse: 0.1835 - output_2_accuracy: 0.9258\n",
      "Epoch 37: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 105s 795ms/step - loss: 0.5487 - output_1_loss: 0.1835 - output_2_loss: 0.1818 - output_1_mse: 0.1835 - output_2_accuracy: 0.9258 - val_loss: 1.1414 - val_output_1_loss: 0.2973 - val_output_2_loss: 0.5468 - val_output_1_mse: 0.2973 - val_output_2_accuracy: 0.8364 - lr: 0.0763\n",
      "Epoch 38/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5435 - output_1_loss: 0.1910 - output_2_loss: 0.1615 - output_1_mse: 0.1910 - output_2_accuracy: 0.9356\n",
      "Epoch 38: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 107s 809ms/step - loss: 0.5435 - output_1_loss: 0.1910 - output_2_loss: 0.1615 - output_1_mse: 0.1910 - output_2_accuracy: 0.9356 - val_loss: 1.2602 - val_output_1_loss: 0.3949 - val_output_2_loss: 0.4705 - val_output_1_mse: 0.3949 - val_output_2_accuracy: 0.8439 - lr: 0.0756\n",
      "Epoch 39/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5676 - output_1_loss: 0.1932 - output_2_loss: 0.1813 - output_1_mse: 0.1932 - output_2_accuracy: 0.9269\n",
      "Epoch 39: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 108s 818ms/step - loss: 0.5676 - output_1_loss: 0.1932 - output_2_loss: 0.1813 - output_1_mse: 0.1932 - output_2_accuracy: 0.9269 - val_loss: 1.1093 - val_output_1_loss: 0.3460 - val_output_2_loss: 0.4173 - val_output_1_mse: 0.3460 - val_output_2_accuracy: 0.8636 - lr: 0.0748\n",
      "Epoch 40/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5550 - output_1_loss: 0.1820 - output_2_loss: 0.1910 - output_1_mse: 0.1820 - output_2_accuracy: 0.9231\n",
      "Epoch 40: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 109s 823ms/step - loss: 0.5550 - output_1_loss: 0.1820 - output_2_loss: 0.1910 - output_1_mse: 0.1820 - output_2_accuracy: 0.9231 - val_loss: 4.0103 - val_output_1_loss: 1.5954 - val_output_2_loss: 0.8194 - val_output_1_mse: 1.5954 - val_output_2_accuracy: 0.7576 - lr: 0.0741\n",
      "Epoch 41/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.4595 - output_1_loss: 0.1564 - output_2_loss: 0.1467 - output_1_mse: 0.1564 - output_2_accuracy: 0.9455\n",
      "Epoch 41: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 109s 826ms/step - loss: 0.4595 - output_1_loss: 0.1564 - output_2_loss: 0.1467 - output_1_mse: 0.1564 - output_2_accuracy: 0.9455 - val_loss: 0.8966 - val_output_1_loss: 0.2855 - val_output_2_loss: 0.3255 - val_output_1_mse: 0.2855 - val_output_2_accuracy: 0.8667 - lr: 0.0733\n",
      "Epoch 42/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5138 - output_1_loss: 0.1653 - output_2_loss: 0.1832 - output_1_mse: 0.1653 - output_2_accuracy: 0.9258\n",
      "Epoch 42: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 110s 831ms/step - loss: 0.5138 - output_1_loss: 0.1653 - output_2_loss: 0.1832 - output_1_mse: 0.1653 - output_2_accuracy: 0.9258 - val_loss: 1.2509 - val_output_1_loss: 0.3743 - val_output_2_loss: 0.5023 - val_output_1_mse: 0.3743 - val_output_2_accuracy: 0.8409 - lr: 0.0726\n",
      "Epoch 43/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.4392 - output_1_loss: 0.1642 - output_2_loss: 0.1109 - output_1_mse: 0.1642 - output_2_accuracy: 0.9617\n",
      "Epoch 43: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 110s 832ms/step - loss: 0.4392 - output_1_loss: 0.1642 - output_2_loss: 0.1109 - output_1_mse: 0.1642 - output_2_accuracy: 0.9617 - val_loss: 1.0275 - val_output_1_loss: 0.2998 - val_output_2_loss: 0.4279 - val_output_1_mse: 0.2998 - val_output_2_accuracy: 0.8636 - lr: 0.0719\n",
      "Epoch 44/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.4442 - output_1_loss: 0.1583 - output_2_loss: 0.1276 - output_1_mse: 0.1583 - output_2_accuracy: 0.9492\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.035588525235652924.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.83374\n",
      "132/132 [==============================] - 110s 830ms/step - loss: 0.4442 - output_1_loss: 0.1583 - output_2_loss: 0.1276 - output_1_mse: 0.1583 - output_2_accuracy: 0.9492 - val_loss: 0.9200 - val_output_1_loss: 0.2963 - val_output_2_loss: 0.3275 - val_output_1_mse: 0.2963 - val_output_2_accuracy: 0.8788 - lr: 0.0712\n",
      "Epoch 45/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.3059 - output_1_loss: 0.1192 - output_2_loss: 0.0675 - output_1_mse: 0.1192 - output_2_accuracy: 0.9780\n",
      "Epoch 45: val_loss improved from 0.83374 to 0.81469, saving model to ./MT-Resnet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 128s 970ms/step - loss: 0.3059 - output_1_loss: 0.1192 - output_2_loss: 0.0675 - output_1_mse: 0.1192 - output_2_accuracy: 0.9780 - val_loss: 0.8147 - val_output_1_loss: 0.2542 - val_output_2_loss: 0.3064 - val_output_1_mse: 0.2542 - val_output_2_accuracy: 0.9045 - lr: 0.0352\n",
      "Epoch 46/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2657 - output_1_loss: 0.1006 - output_2_loss: 0.0644 - output_1_mse: 0.1006 - output_2_accuracy: 0.9773\n",
      "Epoch 46: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 778ms/step - loss: 0.2657 - output_1_loss: 0.1006 - output_2_loss: 0.0644 - output_1_mse: 0.1006 - output_2_accuracy: 0.9773 - val_loss: 1.0641 - val_output_1_loss: 0.3091 - val_output_2_loss: 0.4458 - val_output_1_mse: 0.3091 - val_output_2_accuracy: 0.8773 - lr: 0.0349\n",
      "Epoch 47/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2582 - output_1_loss: 0.0985 - output_2_loss: 0.0612 - output_1_mse: 0.0985 - output_2_accuracy: 0.9807\n",
      "Epoch 47: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 780ms/step - loss: 0.2582 - output_1_loss: 0.0985 - output_2_loss: 0.0612 - output_1_mse: 0.0985 - output_2_accuracy: 0.9807 - val_loss: 0.8714 - val_output_1_loss: 0.2706 - val_output_2_loss: 0.3302 - val_output_1_mse: 0.2706 - val_output_2_accuracy: 0.8985 - lr: 0.0345\n",
      "Epoch 48/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2420 - output_1_loss: 0.0960 - output_2_loss: 0.0500 - output_1_mse: 0.0960 - output_2_accuracy: 0.9848\n",
      "Epoch 48: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 778ms/step - loss: 0.2420 - output_1_loss: 0.0960 - output_2_loss: 0.0500 - output_1_mse: 0.0960 - output_2_accuracy: 0.9848 - val_loss: 1.0352 - val_output_1_loss: 0.3475 - val_output_2_loss: 0.3401 - val_output_1_mse: 0.3475 - val_output_2_accuracy: 0.8939 - lr: 0.0342\n",
      "Epoch 49/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2269 - output_1_loss: 0.0860 - output_2_loss: 0.0550 - output_1_mse: 0.0860 - output_2_accuracy: 0.9814\n",
      "Epoch 49: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 105s 797ms/step - loss: 0.2269 - output_1_loss: 0.0860 - output_2_loss: 0.0550 - output_1_mse: 0.0860 - output_2_accuracy: 0.9814 - val_loss: 0.9305 - val_output_1_loss: 0.2607 - val_output_2_loss: 0.4091 - val_output_1_mse: 0.2607 - val_output_2_accuracy: 0.8864 - lr: 0.0339\n",
      "Epoch 50/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2735 - output_1_loss: 0.0952 - output_2_loss: 0.0831 - output_1_mse: 0.0952 - output_2_accuracy: 0.9686\n",
      "Epoch 50: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 789ms/step - loss: 0.2735 - output_1_loss: 0.0952 - output_2_loss: 0.0831 - output_1_mse: 0.0952 - output_2_accuracy: 0.9686 - val_loss: 0.9697 - val_output_1_loss: 0.3259 - val_output_2_loss: 0.3179 - val_output_1_mse: 0.3259 - val_output_2_accuracy: 0.8879 - lr: 0.0335\n",
      "Epoch 51/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2272 - output_1_loss: 0.0891 - output_2_loss: 0.0490 - output_1_mse: 0.0891 - output_2_accuracy: 0.9856\n",
      "Epoch 51: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 784ms/step - loss: 0.2272 - output_1_loss: 0.0891 - output_2_loss: 0.0490 - output_1_mse: 0.0891 - output_2_accuracy: 0.9856 - val_loss: 0.9122 - val_output_1_loss: 0.2793 - val_output_2_loss: 0.3536 - val_output_1_mse: 0.2793 - val_output_2_accuracy: 0.8924 - lr: 0.0332\n",
      "Epoch 52/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2347 - output_1_loss: 0.0959 - output_2_loss: 0.0429 - output_1_mse: 0.0959 - output_2_accuracy: 0.9856\n",
      "Epoch 52: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 105s 795ms/step - loss: 0.2347 - output_1_loss: 0.0959 - output_2_loss: 0.0429 - output_1_mse: 0.0959 - output_2_accuracy: 0.9856 - val_loss: 0.9175 - val_output_1_loss: 0.2855 - val_output_2_loss: 0.3465 - val_output_1_mse: 0.2855 - val_output_2_accuracy: 0.8818 - lr: 0.0329\n",
      "Epoch 53/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2404 - output_1_loss: 0.1001 - output_2_loss: 0.0403 - output_1_mse: 0.1001 - output_2_accuracy: 0.9867\n",
      "Epoch 53: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 783ms/step - loss: 0.2404 - output_1_loss: 0.1001 - output_2_loss: 0.0403 - output_1_mse: 0.1001 - output_2_accuracy: 0.9867 - val_loss: 1.2273 - val_output_1_loss: 0.2807 - val_output_2_loss: 0.6660 - val_output_1_mse: 0.2807 - val_output_2_accuracy: 0.8545 - lr: 0.0325\n",
      "Epoch 54/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2419 - output_1_loss: 0.0946 - output_2_loss: 0.0527 - output_1_mse: 0.0946 - output_2_accuracy: 0.9784\n",
      "Epoch 54: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 786ms/step - loss: 0.2419 - output_1_loss: 0.0946 - output_2_loss: 0.0527 - output_1_mse: 0.0946 - output_2_accuracy: 0.9784 - val_loss: 0.9726 - val_output_1_loss: 0.2801 - val_output_2_loss: 0.4123 - val_output_1_mse: 0.2801 - val_output_2_accuracy: 0.8879 - lr: 0.0322\n",
      "Epoch 55/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2521 - output_1_loss: 0.0987 - output_2_loss: 0.0547 - output_1_mse: 0.0987 - output_2_accuracy: 0.9799\n",
      "Epoch 55: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 784ms/step - loss: 0.2521 - output_1_loss: 0.0987 - output_2_loss: 0.0547 - output_1_mse: 0.0987 - output_2_accuracy: 0.9799 - val_loss: 0.9037 - val_output_1_loss: 0.2786 - val_output_2_loss: 0.3466 - val_output_1_mse: 0.2786 - val_output_2_accuracy: 0.8939 - lr: 0.0319\n",
      "Epoch 56/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2265 - output_1_loss: 0.0871 - output_2_loss: 0.0523 - output_1_mse: 0.0871 - output_2_accuracy: 0.9795\n",
      "Epoch 56: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 103s 784ms/step - loss: 0.2265 - output_1_loss: 0.0871 - output_2_loss: 0.0523 - output_1_mse: 0.0871 - output_2_accuracy: 0.9795 - val_loss: 1.1150 - val_output_1_loss: 0.2759 - val_output_2_loss: 0.5632 - val_output_1_mse: 0.2759 - val_output_2_accuracy: 0.8530 - lr: 0.0316\n",
      "Epoch 57/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2100 - output_1_loss: 0.0833 - output_2_loss: 0.0433 - output_1_mse: 0.0833 - output_2_accuracy: 0.9826\n",
      "Epoch 57: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 788ms/step - loss: 0.2100 - output_1_loss: 0.0833 - output_2_loss: 0.0433 - output_1_mse: 0.0833 - output_2_accuracy: 0.9826 - val_loss: 0.9389 - val_output_1_loss: 0.2710 - val_output_2_loss: 0.3969 - val_output_1_mse: 0.2710 - val_output_2_accuracy: 0.8742 - lr: 0.0313\n",
      "Epoch 58/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2760 - output_1_loss: 0.0984 - output_2_loss: 0.0792 - output_1_mse: 0.0984 - output_2_accuracy: 0.9708\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.01546958927065134.\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 788ms/step - loss: 0.2760 - output_1_loss: 0.0984 - output_2_loss: 0.0792 - output_1_mse: 0.0984 - output_2_accuracy: 0.9708 - val_loss: 0.9333 - val_output_1_loss: 0.2923 - val_output_2_loss: 0.3487 - val_output_1_mse: 0.2923 - val_output_2_accuracy: 0.8848 - lr: 0.0309\n",
      "Epoch 59/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1515 - output_1_loss: 0.0637 - output_2_loss: 0.0241 - output_1_mse: 0.0637 - output_2_accuracy: 0.9902\n",
      "Epoch 59: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 789ms/step - loss: 0.1515 - output_1_loss: 0.0637 - output_2_loss: 0.0241 - output_1_mse: 0.0637 - output_2_accuracy: 0.9902 - val_loss: 0.8665 - val_output_1_loss: 0.2622 - val_output_2_loss: 0.3421 - val_output_1_mse: 0.2622 - val_output_2_accuracy: 0.9015 - lr: 0.0153\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - ETA: 0s - loss: 0.1160 - output_1_loss: 0.0543 - output_2_loss: 0.0074 - output_1_mse: 0.0543 - output_2_accuracy: 0.9989\n",
      "Epoch 60: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 792ms/step - loss: 0.1160 - output_1_loss: 0.0543 - output_2_loss: 0.0074 - output_1_mse: 0.0543 - output_2_accuracy: 0.9989 - val_loss: 0.9405 - val_output_1_loss: 0.2614 - val_output_2_loss: 0.4177 - val_output_1_mse: 0.2614 - val_output_2_accuracy: 0.8788 - lr: 0.0152\n",
      "Epoch 61/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1141 - output_1_loss: 0.0534 - output_2_loss: 0.0072 - output_1_mse: 0.0534 - output_2_accuracy: 0.9977\n",
      "Epoch 61: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 105s 793ms/step - loss: 0.1141 - output_1_loss: 0.0534 - output_2_loss: 0.0072 - output_1_mse: 0.0534 - output_2_accuracy: 0.9977 - val_loss: 0.9293 - val_output_1_loss: 0.2646 - val_output_2_loss: 0.4002 - val_output_1_mse: 0.2646 - val_output_2_accuracy: 0.8939 - lr: 0.0150\n",
      "Epoch 62/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1022 - output_1_loss: 0.0485 - output_2_loss: 0.0053 - output_1_mse: 0.0485 - output_2_accuracy: 0.9989\n",
      "Epoch 62: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 788ms/step - loss: 0.1022 - output_1_loss: 0.0485 - output_2_loss: 0.0053 - output_1_mse: 0.0485 - output_2_accuracy: 0.9989 - val_loss: 0.8770 - val_output_1_loss: 0.2619 - val_output_2_loss: 0.3533 - val_output_1_mse: 0.2619 - val_output_2_accuracy: 0.9015 - lr: 0.0149\n",
      "Epoch 63/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1084 - output_1_loss: 0.0491 - output_2_loss: 0.0102 - output_1_mse: 0.0491 - output_2_accuracy: 0.9970\n",
      "Epoch 63: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 790ms/step - loss: 0.1084 - output_1_loss: 0.0491 - output_2_loss: 0.0102 - output_1_mse: 0.0491 - output_2_accuracy: 0.9970 - val_loss: 1.0142 - val_output_1_loss: 0.2939 - val_output_2_loss: 0.4264 - val_output_1_mse: 0.2939 - val_output_2_accuracy: 0.8909 - lr: 0.0147\n",
      "Epoch 64/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1072 - output_1_loss: 0.0495 - output_2_loss: 0.0082 - output_1_mse: 0.0495 - output_2_accuracy: 0.9977\n",
      "Epoch 64: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 789ms/step - loss: 0.1072 - output_1_loss: 0.0495 - output_2_loss: 0.0082 - output_1_mse: 0.0495 - output_2_accuracy: 0.9977 - val_loss: 1.0051 - val_output_1_loss: 0.2696 - val_output_2_loss: 0.4659 - val_output_1_mse: 0.2696 - val_output_2_accuracy: 0.9015 - lr: 0.0146\n",
      "Epoch 65/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0961 - output_1_loss: 0.0468 - output_2_loss: 0.0026 - output_1_mse: 0.0468 - output_2_accuracy: 0.9996\n",
      "Epoch 65: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 791ms/step - loss: 0.0961 - output_1_loss: 0.0468 - output_2_loss: 0.0026 - output_1_mse: 0.0468 - output_2_accuracy: 0.9996 - val_loss: 0.9346 - val_output_1_loss: 0.2538 - val_output_2_loss: 0.4270 - val_output_1_mse: 0.2538 - val_output_2_accuracy: 0.8970 - lr: 0.0144\n",
      "Epoch 66/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0911 - output_1_loss: 0.0436 - output_2_loss: 0.0040 - output_1_mse: 0.0436 - output_2_accuracy: 0.9996\n",
      "Epoch 66: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 104s 790ms/step - loss: 0.0911 - output_1_loss: 0.0436 - output_2_loss: 0.0040 - output_1_mse: 0.0436 - output_2_accuracy: 0.9996 - val_loss: 0.9248 - val_output_1_loss: 0.2689 - val_output_2_loss: 0.3869 - val_output_1_mse: 0.2689 - val_output_2_accuracy: 0.9030 - lr: 0.0143\n",
      "Epoch 67/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1111 - output_1_loss: 0.0466 - output_2_loss: 0.0180 - output_1_mse: 0.0466 - output_2_accuracy: 0.9936\n",
      "Epoch 67: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 107s 813ms/step - loss: 0.1111 - output_1_loss: 0.0466 - output_2_loss: 0.0180 - output_1_mse: 0.0466 - output_2_accuracy: 0.9936 - val_loss: 1.0671 - val_output_1_loss: 0.2587 - val_output_2_loss: 0.5497 - val_output_1_mse: 0.2587 - val_output_2_accuracy: 0.8879 - lr: 0.0141\n",
      "Epoch 68/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1470 - output_1_loss: 0.0544 - output_2_loss: 0.0382 - output_1_mse: 0.0544 - output_2_accuracy: 0.9871\n",
      "Epoch 68: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 106s 803ms/step - loss: 0.1470 - output_1_loss: 0.0544 - output_2_loss: 0.0382 - output_1_mse: 0.0544 - output_2_accuracy: 0.9871 - val_loss: 0.9572 - val_output_1_loss: 0.2629 - val_output_2_loss: 0.4314 - val_output_1_mse: 0.2629 - val_output_2_accuracy: 0.8848 - lr: 0.0140\n",
      "Epoch 69/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1342 - output_1_loss: 0.0541 - output_2_loss: 0.0259 - output_1_mse: 0.0541 - output_2_accuracy: 0.9924\n",
      "Epoch 69: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 105s 796ms/step - loss: 0.1342 - output_1_loss: 0.0541 - output_2_loss: 0.0259 - output_1_mse: 0.0541 - output_2_accuracy: 0.9924 - val_loss: 0.9522 - val_output_1_loss: 0.2440 - val_output_2_loss: 0.4642 - val_output_1_mse: 0.2440 - val_output_2_accuracy: 0.8879 - lr: 0.0139\n",
      "Epoch 70/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1430 - output_1_loss: 0.0577 - output_2_loss: 0.0277 - output_1_mse: 0.0577 - output_2_accuracy: 0.9913\n",
      "Epoch 70: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 105s 793ms/step - loss: 0.1430 - output_1_loss: 0.0577 - output_2_loss: 0.0277 - output_1_mse: 0.0577 - output_2_accuracy: 0.9913 - val_loss: 0.8839 - val_output_1_loss: 0.2661 - val_output_2_loss: 0.3516 - val_output_1_mse: 0.2661 - val_output_2_accuracy: 0.9030 - lr: 0.0137\n",
      "Epoch 71/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1277 - output_1_loss: 0.0547 - output_2_loss: 0.0184 - output_1_mse: 0.0547 - output_2_accuracy: 0.9939\n",
      "Epoch 71: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 106s 800ms/step - loss: 0.1277 - output_1_loss: 0.0547 - output_2_loss: 0.0184 - output_1_mse: 0.0547 - output_2_accuracy: 0.9939 - val_loss: 0.9600 - val_output_1_loss: 0.2638 - val_output_2_loss: 0.4324 - val_output_1_mse: 0.2638 - val_output_2_accuracy: 0.9030 - lr: 0.0136\n",
      "Epoch 72/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1192 - output_1_loss: 0.0515 - output_2_loss: 0.0161 - output_1_mse: 0.0515 - output_2_accuracy: 0.9932\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.006724306847900152.\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 101s 767ms/step - loss: 0.1192 - output_1_loss: 0.0515 - output_2_loss: 0.0161 - output_1_mse: 0.0515 - output_2_accuracy: 0.9932 - val_loss: 0.9185 - val_output_1_loss: 0.2624 - val_output_2_loss: 0.3937 - val_output_1_mse: 0.2624 - val_output_2_accuracy: 0.9000 - lr: 0.0134\n",
      "Epoch 73/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0796 - output_1_loss: 0.0379 - output_2_loss: 0.0038 - output_1_mse: 0.0379 - output_2_accuracy: 0.9996\n",
      "Epoch 73: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 99s 747ms/step - loss: 0.0796 - output_1_loss: 0.0379 - output_2_loss: 0.0038 - output_1_mse: 0.0379 - output_2_accuracy: 0.9996 - val_loss: 0.8845 - val_output_1_loss: 0.2623 - val_output_2_loss: 0.3599 - val_output_1_mse: 0.2623 - val_output_2_accuracy: 0.9076 - lr: 0.0067\n",
      "Epoch 74/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0669 - output_1_loss: 0.0321 - output_2_loss: 0.0026 - output_1_mse: 0.0321 - output_2_accuracy: 0.9996\n",
      "Epoch 74: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 101s 762ms/step - loss: 0.0669 - output_1_loss: 0.0321 - output_2_loss: 0.0026 - output_1_mse: 0.0321 - output_2_accuracy: 0.9996 - val_loss: 0.9268 - val_output_1_loss: 0.2706 - val_output_2_loss: 0.3856 - val_output_1_mse: 0.2706 - val_output_2_accuracy: 0.9106 - lr: 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0633 - output_1_loss: 0.0307 - output_2_loss: 0.0019 - output_1_mse: 0.0307 - output_2_accuracy: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.81469\n",
      "132/132 [==============================] - 102s 770ms/step - loss: 0.0633 - output_1_loss: 0.0307 - output_2_loss: 0.0019 - output_1_mse: 0.0307 - output_2_accuracy: 1.0000 - val_loss: 0.8836 - val_output_1_loss: 0.2587 - val_output_2_loss: 0.3662 - val_output_1_mse: 0.2587 - val_output_2_accuracy: 0.9121 - lr: 0.0065\n"
     ]
    }
   ],
   "source": [
    "Loss_Functions = {'output_1': 'mse', 'output_2': 'binary_crossentropy'}\n",
    "Loss_Weights = {'output_1': 2, 'output_2': 1}\n",
    "Metrics= {'output_1': 'mse', 'output_2': 'accuracy'}\n",
    "adam_optimizer = Adam(learning_rate = 0.1)\n",
    "model.compile(optimizer = adam_optimizer, loss = Loss_Functions, loss_weights = Loss_Weights, metrics = Metrics)\n",
    "history = model.fit(img_full_train, [y_full_train[:, 0], y_full_train[:, 2]], batch_size = 20, epochs = 1000, verbose = 1, \n",
    "          callbacks = required_callbacks, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77323200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MT-Resnet/final/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./MT-Resnet/final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861acf67",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215fd0a",
   "metadata": {},
   "source": [
    "## 5.1 Performance on SCUT-FBP Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e727fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 12s 112ms/step - loss: 1.1057 - output_1_loss: 0.3801 - output_2_loss: 0.3455 - output_1_mse: 0.3801 - output_2_accuracy: 0.8882\n",
      "loss of FAP task:  0.38012954592704773\n",
      "loss of gender prediction task:  0.34548962116241455\n",
      "accuracy of gender prediction task:  0.8881818056106567\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(img_test, [y_test[:,0], y_test[:,2]], batch_size=20)\n",
    "print(\"loss of FAP task: \", results[1])\n",
    "print(\"loss of gender prediction task: \", results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0f252ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2label(y_pred):\n",
    "    y_pred_2 = []\n",
    "    for arr in y_pred[1]:\n",
    "        if arr[0] >= 0.5:\n",
    "            y_pred_2.append(1)\n",
    "        else:\n",
    "            y_pred_2.append(0)\n",
    "    \n",
    "    return y_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44d2f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 13s 113ms/step\n",
      "accuracy of gender prediction task:  0.8881818181818182\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(img_test, batch_size=20)\n",
    "y_pred_2 = prob2label(y_pred)\n",
    "print(\"accuracy of gender prediction task: \", accuracy_score(y_true=y_test[:,2], y_pred=y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553cba0",
   "metadata": {},
   "source": [
    "## 5.2 Performance on MEBeauty Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18c167ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 528/528 [00:05<00:00, 100.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data: y_arr = [rating, race, gender]\n",
    "def load_data_MEB(label_dir):\n",
    "    all_ratings = pd.read_csv(label_dir, sep = ' ', header = None)\n",
    "    all_ratings.columns = ['img_path', 'rating']\n",
    "\n",
    "    img_arr = np.zeros([len(all_ratings), 64, 64, 3])\n",
    "    y_arr = np.zeros([len(all_ratings), 3])\n",
    "\n",
    "    for i in tqdm(range(len(all_ratings))):\n",
    "    #for i in tqdm(range(500)):\n",
    "        file_name = all_ratings.iloc[i, 0]\n",
    "        if file_name.split('/')[3] == 'male':\n",
    "            y_arr[i, 2] = 0\n",
    "        else:\n",
    "            y_arr[i, 2] = 1   \n",
    "        y_arr[i, 0] = all_ratings.iloc[i, 1] / 2\n",
    "    \n",
    "        img = tf.io.read_file(file_name)\n",
    "        img = tf.image.decode_jpeg(img, channels = 3)\n",
    "        img = tf.keras.layers.Resizing(64, 64)(img)\n",
    "        img = preprocess_input(img)\n",
    "        img_arr[i] = img\n",
    "\n",
    "    return img_arr, y_arr\n",
    "\n",
    "img_test_meb, y_test_meb = load_data_MEB('./test_2022.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2697229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 111ms/step - loss: 2.8950 - output_1_loss: 0.8137 - output_2_loss: 1.2677 - output_1_mse: 0.8137 - output_2_accuracy: 0.6742\n",
      "loss of FAP task:  0.8136701583862305\n",
      "loss of gender prediction task:  1.2676615715026855\n",
      "accuracy of gender prediction task:  0.6742424368858337\n"
     ]
    }
   ],
   "source": [
    "results_meb = model.evaluate(img_test_meb, [y_test_meb[:,0], y_test_meb[:,2]], batch_size=20)\n",
    "print(\"loss of FAP task: \", results_meb[1])\n",
    "print(\"loss of gender prediction task: \", results_meb[2])\n",
    "print(\"accuracy of gender prediction task: \", results_meb[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06b698b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 110ms/step\n",
      "accuracy of gender prediction task:  0.6742424242424242\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(img_test_meb, batch_size=20)\n",
    "y_pred_2 = prob2label(y_pred)\n",
    "print(\"accuracy of gender prediction task: \", accuracy_score(y_true=y_test_meb[:,2], y_pred=y_pred_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

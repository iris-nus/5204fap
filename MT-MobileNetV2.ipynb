{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet201\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Add, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Layer, MaxPool2D, ReLU, Resizing\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_dir = 'C:\\\\Users\\\\Jing Pengwei\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\SCUT-FBP5500_v2\\\\'\n",
    "scut_dir = 'C:\\\\Users\\\\Jing Pengwei\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\SCUT-FBP\\\\'\n",
    "mebeauty_dir = 'C:\\\\Users\\\\Jing Pengwei\\\\Desktop\\\\DSA5204 Project\\\\Dataset\\\\Facial Attractiveness\\\\MEBeauty\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Background from Images\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "file_list = glob.glob(dataset_dir + 'Images\\\\*')\n",
    "\n",
    "BG_COLOR = (255, 255, 255)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        file_name = file_list[i].split('\\\\')[-1]\n",
    "        image = cv2.imread(file_list[i])\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "        \n",
    "        #fg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        #fg_image[:] = MASK_COLOR\n",
    "        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        bg_image[:] = BG_COLOR\n",
    "\n",
    "        output_image = np.where(condition, image, bg_image)\n",
    "        cv2.imwrite(dataset_dir + 'Cleaned_Images\\\\' + file_name, output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation - MT-ResNet Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv(dataset_dir + 'train_test_files\\\\All_labels.txt', sep = ' ', header = None)\n",
    "all_ratings.columns = ['img_path', 'rating']\n",
    "\n",
    "img_arr = np.zeros([len(all_ratings), 224, 224, 3])\n",
    "y_arr = np.zeros([len(all_ratings), 2])\n",
    "\n",
    "for i in tqdm(range(len(all_ratings))):\n",
    "    file_name = all_ratings.iloc[i, 0]\n",
    "    \n",
    "    race = file_name[0]\n",
    "    gender = file_name[1]\n",
    "    \n",
    "    if gender == 'M':\n",
    "        y_arr[i, 1] = 0\n",
    "    elif gender == 'F':\n",
    "        y_arr[i, 1] = 1\n",
    "        \n",
    "    y_arr[i, 0] = all_ratings.iloc[i, 1]\n",
    "    \n",
    "    img = tf.io.read_file(dataset_dir + 'Cleaned_Images\\\\' + file_name)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.keras.layers.Resizing(224, 224)(img)\n",
    "    img /= 255\n",
    "    img_arr[i] = img\n",
    "    \n",
    "np.save(dataset_dir + 'train_img_arr.npy', img_arr)\n",
    "np.save(dataset_dir + 'train_y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_arr = np.load(dataset_dir + 'train_img_arr.npy')\n",
    "train_y_arr = np.load(dataset_dir + 'train_y_arr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MobileNetV2-MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dropout(Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(Custom_Dropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "        def call(self, inputs, training = None):\n",
    "            if training:\n",
    "                return tf.nn.dropout(inputs, rate = self.rate)\n",
    "            return inputs\n",
    "\n",
    "        \n",
    "        \n",
    "class MobileNetV2_MT_Extension(Model):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV2_MT_Extension, self).__init__()\n",
    "        \n",
    "        self.mobilenetv2 = MobileNetV2(include_top = False)\n",
    "        \n",
    "        self.fc2_rating = Dense(1)\n",
    "        self.fc2_gender = Dense(1, activation = 'sigmoid')\n",
    "        \n",
    "        self.global_avg_pool = GlobalAveragePooling2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.dropout = Custom_Dropout(0.3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        x = self.global_avg_pool(self.mobilenetv2(inputs))\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x_rating = self.fc2_rating(x)\n",
    "        x_gender = self.fc2_gender(x)\n",
    "        \n",
    "        return x_rating, x_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loss Function Definition - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_Functions = {'output_1': 'mse', 'output_2': 'binary_crossentropy'}\n",
    "Loss_Weights = {'output_1': 2, 'output_2': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Callbacks Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.01) \n",
    "\n",
    "    \n",
    "    \n",
    "required_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(custom_scheduler),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5, verbose = 1, cooldown = 2, min_lr = 0.0000001),\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training Parameters Definition - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = Adam(learning_rate = 0.005)\n",
    "model = MobileNetV2_MT_Extension()\n",
    "model.compile(optimizer = adam_optimizer, loss = Loss_Functions, loss_weights = Loss_Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Phase - MT-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "176/176 [==============================] - 27s 98ms/step - loss: 1.9628 - output_1_loss: 0.7772 - output_2_loss: 0.4083 - val_loss: 1324.8024 - val_output_1_loss: 659.2043 - val_output_2_loss: 6.3940 - lr: 0.0050\n",
      "Epoch 2/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.5129 - output_1_loss: 0.1808 - output_2_loss: 0.1512 - val_loss: 1645.6917 - val_output_1_loss: 817.3048 - val_output_2_loss: 11.0817 - lr: 0.0050\n",
      "Epoch 3/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.4238 - output_1_loss: 0.1581 - output_2_loss: 0.1076 - val_loss: 1060.5388 - val_output_1_loss: 523.1833 - val_output_2_loss: 14.1724 - lr: 0.0050\n",
      "Epoch 4/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.3603 - output_1_loss: 0.1461 - output_2_loss: 0.0681 - val_loss: 1556.6373 - val_output_1_loss: 767.0629 - val_output_2_loss: 22.5113 - lr: 0.0050\n",
      "Epoch 5/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2691 - output_1_loss: 0.1072 - output_2_loss: 0.0546 - val_loss: 1137.7996 - val_output_1_loss: 561.7405 - val_output_2_loss: 14.3185 - lr: 0.0050\n",
      "Epoch 6/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.2771 - output_1_loss: 0.1167 - output_2_loss: 0.0436 - val_loss: 419.1331 - val_output_1_loss: 206.3210 - val_output_2_loss: 6.4911 - lr: 0.0050\n",
      "Epoch 7/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2195 - output_1_loss: 0.0958 - output_2_loss: 0.0279 - val_loss: 141.0979 - val_output_1_loss: 69.7749 - val_output_2_loss: 1.5481 - lr: 0.0049\n",
      "Epoch 8/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.2621 - output_1_loss: 0.1043 - output_2_loss: 0.0535 - val_loss: 117.0003 - val_output_1_loss: 58.0352 - val_output_2_loss: 0.9300 - lr: 0.0049\n",
      "Epoch 9/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2280 - output_1_loss: 0.0999 - output_2_loss: 0.0282 - val_loss: 99.5534 - val_output_1_loss: 47.9587 - val_output_2_loss: 3.6360 - lr: 0.0048\n",
      "Epoch 10/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2117 - output_1_loss: 0.0926 - output_2_loss: 0.0265 - val_loss: 9.4577 - val_output_1_loss: 4.6098 - val_output_2_loss: 0.2382 - lr: 0.0048\n",
      "Epoch 11/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2039 - output_1_loss: 0.0850 - output_2_loss: 0.0339 - val_loss: 7.5267 - val_output_1_loss: 3.2682 - val_output_2_loss: 0.9902 - lr: 0.0047\n",
      "Epoch 12/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2057 - output_1_loss: 0.0924 - output_2_loss: 0.0209 - val_loss: 7.4323 - val_output_1_loss: 2.7396 - val_output_2_loss: 1.9531 - lr: 0.0047\n",
      "Epoch 13/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.1698 - output_1_loss: 0.0749 - output_2_loss: 0.0201 - val_loss: 1.3794 - val_output_1_loss: 0.3908 - val_output_2_loss: 0.5978 - lr: 0.0046\n",
      "Epoch 14/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2113 - output_1_loss: 0.0872 - output_2_loss: 0.0369 - val_loss: 0.8178 - val_output_1_loss: 0.2480 - val_output_2_loss: 0.3218 - lr: 0.0046\n",
      "Epoch 15/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.1724 - output_1_loss: 0.0725 - output_2_loss: 0.0273 - val_loss: 1.0162 - val_output_1_loss: 0.4272 - val_output_2_loss: 0.1617 - lr: 0.0045\n",
      "Epoch 16/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.2302 - output_1_loss: 0.0959 - output_2_loss: 0.0383 - val_loss: 0.8870 - val_output_1_loss: 0.2716 - val_output_2_loss: 0.3438 - lr: 0.0045\n",
      "Epoch 17/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.1787 - output_1_loss: 0.0723 - output_2_loss: 0.0342 - val_loss: 0.8836 - val_output_1_loss: 0.2380 - val_output_2_loss: 0.4076 - lr: 0.0044\n",
      "Epoch 18/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.1470 - output_1_loss: 0.0670 - output_2_loss: 0.0131 - val_loss: 1.2407 - val_output_1_loss: 0.5136 - val_output_2_loss: 0.2136 - lr: 0.0044\n",
      "Epoch 19/1000\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1327 - output_1_loss: 0.0643 - output_2_loss: 0.0040\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0021733958274126053.\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.1327 - output_1_loss: 0.0643 - output_2_loss: 0.0040 - val_loss: 1.1930 - val_output_1_loss: 0.5493 - val_output_2_loss: 0.0945 - lr: 0.0043\n",
      "Epoch 20/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0839 - output_1_loss: 0.0400 - output_2_loss: 0.0039 - val_loss: 0.5499 - val_output_1_loss: 0.1818 - val_output_2_loss: 0.1864 - lr: 0.0022\n",
      "Epoch 21/1000\n",
      "176/176 [==============================] - 17s 95ms/step - loss: 0.0714 - output_1_loss: 0.0347 - output_2_loss: 0.0020 - val_loss: 0.3734 - val_output_1_loss: 0.1403 - val_output_2_loss: 0.0927 - lr: 0.0021\n",
      "Epoch 22/1000\n",
      "176/176 [==============================] - 19s 109ms/step - loss: 0.0582 - output_1_loss: 0.0286 - output_2_loss: 0.0010 - val_loss: 0.3174 - val_output_1_loss: 0.1165 - val_output_2_loss: 0.0843 - lr: 0.0021\n",
      "Epoch 23/1000\n",
      "176/176 [==============================] - 17s 96ms/step - loss: 0.0438 - output_1_loss: 0.0216 - output_2_loss: 6.7090e-04 - val_loss: 0.2976 - val_output_1_loss: 0.1126 - val_output_2_loss: 0.0724 - lr: 0.0021\n",
      "Epoch 24/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0427 - output_1_loss: 0.0211 - output_2_loss: 5.3225e-04 - val_loss: 0.3162 - val_output_1_loss: 0.1072 - val_output_2_loss: 0.1017 - lr: 0.0021\n",
      "Epoch 25/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0405 - output_1_loss: 0.0199 - output_2_loss: 6.4560e-04 - val_loss: 0.2451 - val_output_1_loss: 0.1027 - val_output_2_loss: 0.0397 - lr: 0.0020\n",
      "Epoch 26/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0396 - output_1_loss: 0.0195 - output_2_loss: 6.0916e-04 - val_loss: 0.2888 - val_output_1_loss: 0.1270 - val_output_2_loss: 0.0348 - lr: 0.0020\n",
      "Epoch 27/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0348 - output_1_loss: 0.0171 - output_2_loss: 6.5601e-04 - val_loss: 0.2343 - val_output_1_loss: 0.1056 - val_output_2_loss: 0.0230 - lr: 0.0020\n",
      "Epoch 28/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.0381 - output_1_loss: 0.0189 - output_2_loss: 2.7523e-04 - val_loss: 0.4529 - val_output_1_loss: 0.2094 - val_output_2_loss: 0.0341 - lr: 0.0020\n",
      "Epoch 29/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.0450 - output_1_loss: 0.0223 - output_2_loss: 3.5602e-04 - val_loss: 0.2487 - val_output_1_loss: 0.1046 - val_output_2_loss: 0.0394 - lr: 0.0020\n",
      "Epoch 30/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.0376 - output_1_loss: 0.0186 - output_2_loss: 4.3044e-04 - val_loss: 0.2807 - val_output_1_loss: 0.1175 - val_output_2_loss: 0.0456 - lr: 0.0019\n",
      "Epoch 31/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0389 - output_1_loss: 0.0193 - output_2_loss: 2.4996e-04 - val_loss: 0.2760 - val_output_1_loss: 0.1135 - val_output_2_loss: 0.0490 - lr: 0.0019\n",
      "Epoch 32/1000\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0366 - output_1_loss: 0.0182 - output_2_loss: 2.1579e-04\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0009542247862555087.\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.0366 - output_1_loss: 0.0182 - output_2_loss: 2.1579e-04 - val_loss: 0.2510 - val_output_1_loss: 0.0994 - val_output_2_loss: 0.0522 - lr: 0.0019\n",
      "Epoch 33/1000\n",
      "176/176 [==============================] - 16s 90ms/step - loss: 0.0254 - output_1_loss: 0.0126 - output_2_loss: 1.9275e-04 - val_loss: 0.2344 - val_output_1_loss: 0.1011 - val_output_2_loss: 0.0322 - lr: 9.4473e-04\n",
      "Epoch 34/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.0234 - output_1_loss: 0.0116 - output_2_loss: 1.4895e-04 - val_loss: 0.2393 - val_output_1_loss: 0.1053 - val_output_2_loss: 0.0287 - lr: 9.3533e-04\n",
      "Epoch 35/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0195 - output_1_loss: 0.0097 - output_2_loss: 1.7032e-04 - val_loss: 0.2219 - val_output_1_loss: 0.0976 - val_output_2_loss: 0.0268 - lr: 9.2602e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0178 - output_1_loss: 0.0088 - output_2_loss: 1.6103e-04 - val_loss: 0.2098 - val_output_1_loss: 0.0906 - val_output_2_loss: 0.0287 - lr: 9.1681e-04\n",
      "Epoch 37/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0172 - output_1_loss: 0.0085 - output_2_loss: 1.5221e-04 - val_loss: 0.2187 - val_output_1_loss: 0.0962 - val_output_2_loss: 0.0262 - lr: 9.0769e-04\n",
      "Epoch 38/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.0176 - output_1_loss: 0.0085 - output_2_loss: 4.9167e-04 - val_loss: 0.2243 - val_output_1_loss: 0.0943 - val_output_2_loss: 0.0358 - lr: 8.9866e-04\n",
      "Epoch 39/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0155 - output_1_loss: 0.0076 - output_2_loss: 2.9098e-04 - val_loss: 0.2367 - val_output_1_loss: 0.0965 - val_output_2_loss: 0.0436 - lr: 8.8971e-04\n",
      "Epoch 40/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0187 - output_1_loss: 0.0093 - output_2_loss: 1.4367e-04 - val_loss: 0.2269 - val_output_1_loss: 0.0975 - val_output_2_loss: 0.0319 - lr: 8.8086e-04\n",
      "Epoch 41/1000\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0369 - output_1_loss: 0.0181 - output_2_loss: 7.4515e-04\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00043604790698736906.\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0369 - output_1_loss: 0.0181 - output_2_loss: 7.4515e-04 - val_loss: 0.2600 - val_output_1_loss: 0.1119 - val_output_2_loss: 0.0362 - lr: 8.7210e-04\n",
      "Epoch 42/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0250 - output_1_loss: 0.0124 - output_2_loss: 1.7398e-04 - val_loss: 0.2332 - val_output_1_loss: 0.0997 - val_output_2_loss: 0.0338 - lr: 4.3171e-04\n",
      "Epoch 43/1000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.0147 - output_1_loss: 0.0073 - output_2_loss: 1.3742e-04 - val_loss: 0.2227 - val_output_1_loss: 0.0946 - val_output_2_loss: 0.0334 - lr: 4.2741e-04\n",
      "Epoch 44/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0102 - output_1_loss: 0.0050 - output_2_loss: 1.2958e-04 - val_loss: 0.2228 - val_output_1_loss: 0.0975 - val_output_2_loss: 0.0277 - lr: 4.2316e-04\n",
      "Epoch 45/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0093 - output_1_loss: 0.0046 - output_2_loss: 1.0908e-04 - val_loss: 0.2183 - val_output_1_loss: 0.0928 - val_output_2_loss: 0.0328 - lr: 4.1895e-04\n",
      "Epoch 46/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0088 - output_1_loss: 0.0044 - output_2_loss: 1.0031e-04 - val_loss: 0.2232 - val_output_1_loss: 0.0951 - val_output_2_loss: 0.0329 - lr: 4.1478e-04\n",
      "Epoch 47/1000\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0095 - output_1_loss: 0.0047 - output_2_loss: 9.8979e-05\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002053272328339517.\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0095 - output_1_loss: 0.0047 - output_2_loss: 9.8979e-05 - val_loss: 0.2382 - val_output_1_loss: 0.1031 - val_output_2_loss: 0.0320 - lr: 4.1065e-04\n",
      "Epoch 48/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0072 - output_1_loss: 0.0036 - output_2_loss: 1.0027e-04 - val_loss: 0.2335 - val_output_1_loss: 0.1023 - val_output_2_loss: 0.0290 - lr: 2.0328e-04\n",
      "Epoch 49/1000\n",
      "176/176 [==============================] - 16s 91ms/step - loss: 0.0062 - output_1_loss: 0.0030 - output_2_loss: 8.8917e-05 - val_loss: 0.2397 - val_output_1_loss: 0.1051 - val_output_2_loss: 0.0295 - lr: 2.0126e-04\n",
      "Epoch 50/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0047 - output_1_loss: 0.0023 - output_2_loss: 9.0015e-05 - val_loss: 0.2175 - val_output_1_loss: 0.0952 - val_output_2_loss: 0.0272 - lr: 1.9926e-04\n",
      "Epoch 51/1000\n",
      "176/176 [==============================] - 16s 92ms/step - loss: 0.0050 - output_1_loss: 0.0025 - output_2_loss: 9.0572e-05 - val_loss: 0.2217 - val_output_1_loss: 0.0969 - val_output_2_loss: 0.0280 - lr: 1.9728e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a7eecdc460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_img_arr, [train_y_arr[:, 0], train_y_arr[:, 1]], batch_size = 25, \n",
    "          epochs = 1000, verbose = 1, callbacks = required_callbacks, \n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.Custom_Dropout object at 0x000002A652A6C940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MobileNetV2_MT\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MobileNetV2_MT\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('MobileNetV2_MT', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SCUT-FBP Test Dataset Preparation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "rating_df = pd.read_excel(scut_dir + 'Rating_Collection\\\\Attractiveness label.xlsx', header = 0)\n",
    "img_arr = np.zeros([len(rating_df), 224, 224, 3])\n",
    "y_arr = np.zeros([len(rating_df), 2])\n",
    "\n",
    "for i in tqdm(range(len(rating_df))):\n",
    "    num = rating_df.iloc[i, 0]\n",
    "    img = cv2.imread(scut_dir + 'Data_Collection\\\\SCUT-FBP-' + str(num) + '.jpg')\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "        results = selfie_segmentation.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        condition = np.stack((results.segmentation_mask, ) * 3, axis = -1) > 0.1\n",
    "        bg_image = np.zeros(img.shape, dtype = np.uint8)\n",
    "        bg_image[:] = (255, 255, 255)\n",
    "        cleaned_img = np.where(condition, img, bg_image)\n",
    "    \n",
    "    cleaned_img = tf.keras.layers.Resizing(224, 224)(cleaned_img)\n",
    "    cleaned_img /= 255\n",
    "    img_arr[i] = cleaned_img\n",
    "    \n",
    "    y_arr[i, 0] = rating_df.iloc[i, 1]\n",
    "    y_arr[i, 1] = 1\n",
    "    \n",
    "np.save(scut_dir + 'img_arr.npy', img_arr)\n",
    "np.save(scut_dir + 'y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.load(scut_dir + 'img_arr.npy')\n",
    "y_arr = np.load(scut_dir + 'y_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 28ms/step - loss: 0.2966 - output_1_loss: 0.1438 - output_2_loss: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29658475518226624, 0.1437881737947464, 0.009008388966321945]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(img_arr, [y_arr[:, 0], y_arr[:, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MEBeauty Test Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "rating_df = pd.read_csv(mebeauty_dir + 'landmarks.csv', header = 0)\n",
    "#img_arr = np.zeros([len(rating_df), 224, 224, 3])\n",
    "#y_arr = np.zeros([len(rating_df), 2])\n",
    "\n",
    "img_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in tqdm(range(len(rating_df))):\n",
    "    file_path = rating_df.iloc[i, 1]\n",
    "    file_name = '\\\\'.join(file_path.split('/')[5:])\n",
    "    gender = file_path.split('/')[5]\n",
    "        \n",
    "    try:\n",
    "        img = cv2.imread(mebeauty_dir + 'cropped_images\\\\images_crop_align_mtcnn\\\\' + file_name)\n",
    "        '''\n",
    "        with mp_selfie_segmentation.SelfieSegmentation(model_selection = 0) as selfie_segmentation:\n",
    "            results = selfie_segmentation.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            condition = np.stack((results.segmentation_mask, ) * 3, axis = -1) > 0.1\n",
    "            bg_image = np.zeros(img.shape, dtype = np.uint8)\n",
    "            bg_image[:] = (255, 255, 255)\n",
    "            cleaned_img = np.where(condition, img, bg_image)\n",
    "        '''\n",
    "\n",
    "        img = tf.keras.layers.Resizing(224, 224)(img)\n",
    "        img /= 255\n",
    "        #img_arr[i] = img\n",
    "        img = img[np.newaxis, :, :, :]\n",
    "        img_list.append(img)\n",
    "        \n",
    "        if gender == 'female':\n",
    "            #y_arr[i, 1] = 1\n",
    "            y_list.append(np.array([rating_df.iloc[i, 2] / 2, 1]).reshape(1, 2))\n",
    "        else:\n",
    "            #y_arr[i, 1] = 0\n",
    "            y_list.append(np.array([rating_df.iloc[i, 2] / 2, 0]).reshape(1, 2))\n",
    "            \n",
    "        #y_arr[i, 0] = rating_df.iloc[i, 2]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "img_arr = np.concatenate(img_list, axis = 0)\n",
    "y_arr = np.concatenate(y_list, axis = 0)\n",
    "    \n",
    "np.save(mebeauty_dir + 'img_arr.npy', img_arr)\n",
    "np.save(mebeauty_dir + 'y_arr.npy', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.load(mebeauty_dir + 'img_arr.npy')\n",
    "y_arr = np.load(mebeauty_dir + 'y_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 2s 28ms/step - loss: 1.7622 - output_1_loss: 0.4676 - output_2_loss: 0.8271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7621889114379883, 0.46755269169807434, 0.8270841836929321]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(img_arr, [y_arr[:, 0], y_arr[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
